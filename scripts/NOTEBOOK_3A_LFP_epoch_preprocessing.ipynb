{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "from mne.io import read_raw\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import join\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "from functions import ephy_plotting, preprocessing, analysis, io, utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load the dataset #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_path = os.path.dirname(os.getcwd())\n",
    "results_path = join(working_path, \"results\")\n",
    "behav_results_saving_path = join(results_path, \"behav_results\")\n",
    "# read the json file containing the included and excluded subjects, based on the behavioral results\n",
    "included_excluded_file = join(behav_results_saving_path, 'final_included_subjects.json')\n",
    "with open(included_excluded_file, 'r') as file:\n",
    "    included_subjects = json.load(file)\n",
    "\n",
    "# keep only subjects starting with \"sub\":\n",
    "included_subjects = [subj for subj in included_subjects if subj.startswith('sub')]\n",
    "print(f'Included_subjects: {included_subjects}')\n",
    "onedrive_path = utils._get_onedrive_path()\n",
    "\n",
    "#  Set saving path for cleaned epochs\n",
    "saving_path= join(results_path, 'lfp_epochs')\n",
    "os.makedirs(saving_path, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "\n",
    "sub_dict_epochs = {}  #  Stores the epochs for each subject/session\n",
    "all_sub_session_dict = {}\n",
    "all_sub_session_dict = defaultdict(dict)  # Each missing key gets an empty dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load all data for all included subjects\n",
    "# data = io.load_behav_data(included_subjects, onedrive_path)\n",
    "\n",
    "# # Compute statistics for each loaded subject\n",
    "# stats = {}\n",
    "# stats = utils.extract_stats(data)\n",
    "# # If no file was found, create a new JSON file\n",
    "# filename = \"stats.json\"\n",
    "# file_path = os.path.join(results_path, filename)\n",
    "# #if not os.path.isfile(file_path):\n",
    "# #    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "# #            json.dump({}, file, indent=4)\n",
    "\n",
    "# # Save the updated or new JSON file\n",
    "# with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "#     json.dump(stats, file, indent=4)\n",
    "\n",
    "# # remove sub027 DBS OFF mSST from included_subjects because it has not been synchronized yet\n",
    "# #included_subjects.remove('sub027 DBS OFF mSST')\n",
    "# included_subjects\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Create full session plots for each subject (Raw traces, TFR plot, PSD) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for session_ID in included_subjects:\n",
    "    print(f\"Now processing {session_ID}\")\n",
    "    session_dict = {}\n",
    "    sub = session_ID[:6]\n",
    "    condition = session_ID.split(' ') [1] + ' ' + session_ID.split(' ') [2]\n",
    "    sub_onedrive_path = join(onedrive_path, sub)\n",
    "    sub_onedrive_path_task = join(onedrive_path, sub, 'synced_data', session_ID)\n",
    "    filename = [f for f in os.listdir(sub_onedrive_path_task) if (\n",
    "        f.endswith('.set') and f.startswith('SYNCHRONIZED_INTRACRANIAL'))]\n",
    "    \n",
    "    if not filename:\n",
    "        raise FileNotFoundError(f\"No .set file found in {sub_onedrive_path_task}\")\n",
    "\n",
    "    file = join(sub_onedrive_path_task, filename[0])\n",
    "\n",
    "    if not os.path.isfile(file):\n",
    "        raise FileNotFoundError(f\"File does not exist: {file}\")\n",
    "\n",
    "    print(f\"Loading file: {file}\")\n",
    "    #file = join(sub_onedrive_path_task, filename[0])\n",
    "    raw = read_raw(file, preload=True)\n",
    "\n",
    "    saving_path_single = join(results_path, 'single_sub', f'{sub} mSST') \n",
    "    os.makedirs(saving_path_single, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "\n",
    "    ephy_plotting.plot_raw_stim(session_ID, raw, saving_path_single)\n",
    "    psd_left, freqs_left, psd_right, freqs_right = analysis.compute_psd_welch(raw)\n",
    "    session_dict['psd_left_V^2/Hz'] = psd_left\n",
    "    session_dict['freqs_left'] = freqs_left\n",
    "    session_dict['psd_right_V^2/Hz'] = psd_right\n",
    "    session_dict['freqs_right'] = freqs_right\n",
    "\n",
    "    # Compute band power for theta, alpha, low-beta and high-beta ranges:\n",
    "    band_metrics_left = utils.compute_band_metrics(psd_left, freqs_left)\n",
    "    band_metrics_right = utils.compute_band_metrics(psd_right, freqs_right)\n",
    "    session_dict['left'] = band_metrics_left\n",
    "    session_dict['right'] = band_metrics_right\n",
    "\n",
    "    print(f'Values for Left STN: {band_metrics_left}')\n",
    "    print(f'Values for Right STN: {band_metrics_right}')\n",
    "\n",
    "    ephy_plotting.plot_psd_log(\n",
    "        session_ID, raw, freqs_left, psd_left, \n",
    "        freqs_right, psd_right, saving_path_single, is_filt=False\n",
    "        )\n",
    "    ephy_plotting.plot_stft_stim(\n",
    "        session_ID, raw, saving_path=saving_path_single, is_filt=False, \n",
    "        vmin = -18, vmax = -12, \n",
    "        fmin=0, fmax=100\n",
    "        )\n",
    "\n",
    "    all_sub_session_dict[sub][condition] = session_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = analysis.compare_band_power(all_sub_session_dict)\n",
    "# save dataframe to excel\n",
    "df.to_excel(join(results_path, \"band_power_comparison.xlsx\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.stats import wilcoxon\n",
    "\n",
    "# for band in df['band'].unique():\n",
    "#     for hemi in df['hemisphere'].unique():\n",
    "#         subset = df[(df['band'] == band) & (df['hemisphere'] == hemi)]\n",
    "#         stat, p_val = wilcoxon(subset['DBS OFF_power_uV2'], subset['DBS ON_power_uV2'])\n",
    "#         print(f\"{band} - {hemi}: Wilcoxon p={p_val:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Work with epochs #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for session_ID in included_subjects:\n",
    "    session_dict = {}\n",
    "    sub = session_ID[:6]\n",
    "    condition = session_ID.split(' ') [1] + ' ' + session_ID.split(' ') [2]\n",
    "    print(f\"Now processing {session_ID}\")\n",
    "    all_sub_session_dict[sub][condition] = session_dict\n",
    "    \n",
    "    sub_onedrive_path = join(onedrive_path, sub)\n",
    "    sub_onedrive_path_task = join(onedrive_path, sub, 'synced_data', session_ID)\n",
    "    filename = [f for f in os.listdir(sub_onedrive_path_task) if (\n",
    "        f.endswith('.set') and f.startswith('SYNCHRONIZED_INTRACRANIAL'))]\n",
    "    \n",
    "    if not filename:\n",
    "        raise FileNotFoundError(f\"No .set file found in {sub_onedrive_path_task}\")\n",
    "\n",
    "    file = join(sub_onedrive_path_task, filename[0])\n",
    "\n",
    "    if not os.path.isfile(file):\n",
    "        raise FileNotFoundError(f\"File does not exist: {file}\")\n",
    "\n",
    "    print(f\"Loading file: {file}\")\n",
    "    #file = join(sub_onedrive_path_task, filename[0])\n",
    "    raw = read_raw(file, preload=True)\n",
    "    all_sub_session_dict[sub][condition]['CHANNELS'] = raw.ch_names\n",
    "\n",
    "    # Rename channels to be consistent across subjects:\n",
    "    new_channel_names = [\n",
    "        \"Left_STN\",\n",
    "        \"Right_STN\",\n",
    "        \"left_peak_STN\",\n",
    "        \"right_peak_STN\",\n",
    "        \"STIM_Left_STN\",\n",
    "        \"STIM_Right_STN\"\n",
    "    ]\n",
    "\n",
    "    # Get the existing channel names\n",
    "    old_channel_names = raw.ch_names\n",
    "\n",
    "    # Create a mapping from old to new names\n",
    "    rename_dict = {old: new for old, new in zip(old_channel_names, new_channel_names)}\n",
    "\n",
    "    # Rename the channels\n",
    "    raw.rename_channels(rename_dict)\n",
    "\n",
    "    all_sub_session_dict[sub][condition]['RENAMED_CHANNELS'] = raw.ch_names\n",
    "\n",
    "    # Filter between 1 and 80 Hz:\n",
    "    filtered_data = raw.copy().filter(l_freq=1, h_freq=80)\n",
    "\n",
    "    # Extract events and create epochs\n",
    "    # only keep lfp channels\n",
    "    filtered_data_lfp = filtered_data.copy().pick_channels([filtered_data.ch_names[0], filtered_data.ch_names[1]])\n",
    "\n",
    "    #epochs, filtered_event_dict = preprocessing.create_epochs(filtered_data_lfp, session_ID)\n",
    "\n",
    "    mSST_raw_behav_session_data_path = join(\n",
    "            onedrive_path, sub, \"raw_data\", 'BEHAVIOR', condition, 'mSST'\n",
    "            )\n",
    "    for filename in os.listdir(mSST_raw_behav_session_data_path):\n",
    "            if filename.endswith(\".csv\"):\n",
    "                fname = filename\n",
    "    filepath_behav = join(mSST_raw_behav_session_data_path, fname)\n",
    "    print(filepath_behav)\n",
    "    df = pd.read_csv(filepath_behav)\n",
    "\n",
    "    # return the index of the first row which is not filled by a Nan value:\n",
    "    start_task_index = df['blocks.thisRepN'].first_valid_index()\n",
    "    stop_task_index = df['blocks.thisRepN'].last_valid_index()\n",
    "    df_maintask = df.iloc[start_task_index:stop_task_index + 1] ### HERE MISTAKE OF INDEXING: CHECK IN OTHER SCRIPTS IF THIS IS ALSO WRONG!!!\n",
    "\n",
    "\n",
    "    # remove all useless columns to clean up dataframe\n",
    "    column_names = df_maintask.columns\n",
    "    columns_to_keep = [i for i in [\n",
    "        'blocks.thisN', 'trial_loop.thisN', 'trial_type', \n",
    "        'continue_signal_time', 'stop_signal_time', \n",
    "        'key_resp_experiment.keys', 'key_resp_experiment.corr', 'key_resp_experiment.rt',\n",
    "        'early_press_resp.keys', 'early_press_resp.rt', 'early_press_resp.corr',\n",
    "        'late_key_resp1.keys', 'late_key_resp1.rt', \n",
    "        'late_key_resp2.keys', 'late_key_resp2.rt'\n",
    "        ] if i in column_names]\n",
    "\n",
    "    mini_df_maintask = df_maintask[columns_to_keep]\n",
    "    print(mini_df_maintask.shape)\n",
    "\n",
    "    # remove the trials with early presses, as in these trials the cues were not presented (for mSST)\n",
    "    early_presses = mini_df_maintask[mini_df_maintask['early_press_resp.corr'] == 1]\n",
    "    early_presses_trials = list(early_presses.index)\n",
    "    number_early_presses = len(early_presses_trials)\n",
    "    print(f'Number of early presses: {number_early_presses}')\n",
    "\n",
    "    # remove trials with early presses from the dataframe:\n",
    "    df_maintask_copy = mini_df_maintask.drop(early_presses_trials).reset_index(drop=True)\n",
    "    print(df_maintask_copy.shape)\n",
    "    print(df_maintask_copy['blocks.thisN'])\n",
    "\n",
    "    # First generate global epochs (without taking into account success outcome)\n",
    "    # events and event_id used for epochs creation\n",
    "    events, event_id = mne.events_from_annotations(filtered_data_lfp)\n",
    "    epochs, filtered_event_dict = preprocessing.create_epochs(\n",
    "         filtered_data_lfp, \n",
    "         sub, \n",
    "         keys_to_keep = ['GC', 'GF', 'GO', 'GS', 'continue', 'stop'],\n",
    "         tmin = -3.5,\n",
    "         tmax = 3.5,\n",
    "         baseline=None\n",
    "         )\n",
    "    n_epochs = len(epochs)\n",
    "    print(epochs)\n",
    "\n",
    "    # inverse mapping (event code -> label)\n",
    "    inv_event_id = {v: k for k, v in event_id.items()}\n",
    "\n",
    "    metadata = pd.DataFrame(index=np.arange(len(epochs)))\n",
    "    metadata[\"event\"] = [inv_event_id[e] for e in epochs.events[:, 2]]\n",
    "    metadata[\"trial_type\"] = np.nan\n",
    "\n",
    "    # LFP -> behavioral naming mapping\n",
    "    mapping = {\n",
    "        \"GC\": \"go_continue_trial\",\n",
    "        \"GO\": \"go_trial\",\n",
    "        \"GF\": \"go_fast_trial\",\n",
    "        \"GS\": \"stop_trial\",\n",
    "    }\n",
    "\n",
    "    trial_mask = metadata[\"event\"].isin(mapping.keys())\n",
    "\n",
    "    assert trial_mask.sum() == len(df_maintask_copy), \\\n",
    "        f\"Mismatch: {trial_mask.sum()} LFP trials vs {len(df_maintask_copy)} behavioral trials\"\n",
    "\n",
    "    # fill directly from behavioral file\n",
    "    for col in df_maintask_copy.columns:\n",
    "        metadata.loc[trial_mask, col] = df_maintask_copy[col].values\n",
    "\n",
    "    for i in metadata.index:\n",
    "        if metadata.loc[i, \"event\"] == \"continue\":\n",
    "            # find the last GC before this\n",
    "            prev_idx = metadata.loc[:i-1][metadata[\"event\"] == \"GC\"].index[-1]\n",
    "            metadata.loc[i, df_maintask_copy.columns] = metadata.loc[prev_idx, df_maintask_copy.columns]\n",
    "\n",
    "        elif metadata.loc[i, \"event\"] == \"stop\":\n",
    "            # find the last GS before this\n",
    "            prev_idx = metadata.loc[:i-1][metadata[\"event\"] == \"GS\"].index[-1]\n",
    "            metadata.loc[i, df_maintask_copy.columns] = metadata.loc[prev_idx, df_maintask_copy.columns]\n",
    "\n",
    "    epochs.metadata = metadata\n",
    "\n",
    "    sub_dict_epochs[session_ID] = epochs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For each session, look at the epochs and remove 'bad' epochs, then save the cleaned file #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_id = 'sub006 DBS OFF mSST'\n",
    "cleaned_epochs = sub_dict_epochs[subject_id].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "# cleaned_epochs.plot(n_epochs=10, n_channels = len(cleaned_epochs.ch_names), events=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nan_bads = [252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272 ,273, 274, 275, 276, 277, 278, 279, 280]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned_epochs.drop(nan_bads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "cleaned_epochs.plot(n_epochs=4, n_channels = len(cleaned_epochs.ch_names), events=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_epochs.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df = pd.DataFrame(cleaned_epochs.metadata)\n",
    "# save both to csv (easier for later python import), and xlsx (easier to read in excel)\n",
    "metadata_df.to_csv(os.path.join(saving_path, f\"{subject_id}_cleaned-long-epo_metadata.csv\"), index=True)\n",
    "metadata_df.to_excel(os.path.join(saving_path, f\"{subject_id}_cleaned-long-epo_metadata.xlsx\"), index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_epoch = os.path.join(saving_path, f\"{subject_id}_cleaned-long-epo.fif\")\n",
    "cleaned_epochs.save(file_epoch, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#epoch_reload = mne.read_epochs(os.path.join(saving_path, f\"sub011 DBS OFF mSST_cleaned-long-epo.fif\"), preload=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cropped_epochs = cleaned_epochs.copy().crop(tmin=-0.5, tmax=1.5)\n",
    "# file_cropped_epoch = os.path.join(saving_path, f\"{subject_id}_cleaned-short-epo.fif\")\n",
    "# cropped_epochs.save(file_cropped_epoch, overwrite=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ephy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
