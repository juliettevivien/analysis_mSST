{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "from mne.io import read_raw\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import join\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "from functions import ephy_plotting, preprocessing, analysis, io, utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load the dataset #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_path = os.path.dirname(os.getcwd())\n",
    "results_path = join(working_path, \"results\")\n",
    "behav_results_saving_path = join(results_path, \"behav_results\")\n",
    "# read the json file containing the included and excluded subjects, based on the behavioral results\n",
    "included_excluded_file = join(behav_results_saving_path, 'final_included_subjects.json')\n",
    "with open(included_excluded_file, 'r') as file:\n",
    "    included_subjects = json.load(file)\n",
    "\n",
    "# keep only subjects starting with \"sub\":\n",
    "included_subjects = [subj for subj in included_subjects if subj.startswith('sub')]\n",
    "print(f'Included_subjects: {included_subjects}')\n",
    "onedrive_path = utils._get_onedrive_path()\n",
    "\n",
    "#  Set saving path for cleaned epochs\n",
    "saving_path= join(results_path, 'lfp_epochs')\n",
    "os.makedirs(saving_path, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "\n",
    "sub_dict_epochs = {}  #  Stores the epochs for each subject/session\n",
    "all_sub_session_dict = {}\n",
    "all_sub_session_dict = defaultdict(dict)  # Each missing key gets an empty dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "included_subjects.remove('sub028 DBS OFF mSST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load all data for all included subjects\n",
    "# data = io.load_behav_data(included_subjects, onedrive_path)\n",
    "\n",
    "# # Compute statistics for each loaded subject\n",
    "# stats = {}\n",
    "# stats = utils.extract_stats(data)\n",
    "# # If no file was found, create a new JSON file\n",
    "# filename = \"stats.json\"\n",
    "# file_path = os.path.join(results_path, filename)\n",
    "# #if not os.path.isfile(file_path):\n",
    "# #    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "# #            json.dump({}, file, indent=4)\n",
    "\n",
    "# # Save the updated or new JSON file\n",
    "# with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "#     json.dump(stats, file, indent=4)\n",
    "\n",
    "# # remove sub027 DBS OFF mSST from included_subjects because it has not been synchronized yet\n",
    "# #included_subjects.remove('sub027 DBS OFF mSST')\n",
    "# included_subjects\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Create full session plots for each subject (Raw traces, TFR plot, PSD) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for session_ID in included_subjects:\n",
    "    print(f\"Now processing {session_ID}\")\n",
    "    session_dict = {}\n",
    "    sub = session_ID[:6]\n",
    "    condition = session_ID.split(' ') [1] + ' ' + session_ID.split(' ') [2]\n",
    "    sub_onedrive_path = join(onedrive_path, sub)\n",
    "    sub_onedrive_path_task = join(onedrive_path, sub, 'synced_data', session_ID)\n",
    "    filename = [f for f in os.listdir(sub_onedrive_path_task) if (\n",
    "        f.endswith('.set') and f.startswith('SYNCHRONIZED_INTRACRANIAL'))]\n",
    "    \n",
    "    if not filename:\n",
    "        raise FileNotFoundError(f\"No .set file found in {sub_onedrive_path_task}\")\n",
    "\n",
    "    file = join(sub_onedrive_path_task, filename[0])\n",
    "\n",
    "    if not os.path.isfile(file):\n",
    "        raise FileNotFoundError(f\"File does not exist: {file}\")\n",
    "\n",
    "    print(f\"Loading file: {file}\")\n",
    "    #file = join(sub_onedrive_path_task, filename[0])\n",
    "    raw = read_raw(file, preload=True)\n",
    "\n",
    "    saving_path_single = join(results_path, 'single_sub', f'{sub} mSST') \n",
    "    os.makedirs(saving_path_single, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "\n",
    "    ephy_plotting.plot_raw_stim(session_ID, raw, saving_path_single)\n",
    "    psd_left, freqs_left, psd_right, freqs_right = analysis.compute_psd_welch(raw)\n",
    "    session_dict['psd_left_V^2/Hz'] = psd_left\n",
    "    session_dict['freqs_left'] = freqs_left\n",
    "    session_dict['psd_right_V^2/Hz'] = psd_right\n",
    "    session_dict['freqs_right'] = freqs_right\n",
    "\n",
    "    # Compute band power for theta, alpha, low-beta and high-beta ranges:\n",
    "    band_metrics_left = utils.compute_band_metrics(psd_left, freqs_left)\n",
    "    band_metrics_right = utils.compute_band_metrics(psd_right, freqs_right)\n",
    "    session_dict['left'] = band_metrics_left\n",
    "    session_dict['right'] = band_metrics_right\n",
    "\n",
    "    print(f'Values for Left STN: {band_metrics_left}')\n",
    "    print(f'Values for Right STN: {band_metrics_right}')\n",
    "\n",
    "    ephy_plotting.plot_psd_log(\n",
    "        session_ID, raw, freqs_left, psd_left, \n",
    "        freqs_right, psd_right, saving_path_single, is_filt=False\n",
    "        )\n",
    "    ephy_plotting.plot_stft_stim(\n",
    "        session_ID, raw, saving_path=saving_path_single, is_filt=False, \n",
    "        vmin = -18, vmax = -12, \n",
    "        fmin=0, fmax=100\n",
    "        )\n",
    "\n",
    "    all_sub_session_dict[sub][condition] = session_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = analysis.compare_band_power(all_sub_session_dict)\n",
    "# save dataframe to excel\n",
    "df.to_excel(join(results_path, \"band_power_comparison.xlsx\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.stats import wilcoxon\n",
    "\n",
    "# for band in df['band'].unique():\n",
    "#     for hemi in df['hemisphere'].unique():\n",
    "#         subset = df[(df['band'] == band) & (df['hemisphere'] == hemi)]\n",
    "#         stat, p_val = wilcoxon(subset['DBS OFF_power_uV2'], subset['DBS ON_power_uV2'])\n",
    "#         print(f\"{band} - {hemi}: Wilcoxon p={p_val:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Work with epochs #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for session_ID in included_subjects:\n",
    "    session_dict = {}\n",
    "    sub = session_ID[:6]\n",
    "    condition = session_ID.split(' ') [1] + ' ' + session_ID.split(' ') [2]\n",
    "    print(f\"Now processing {session_ID}\")\n",
    "    all_sub_session_dict[sub][condition] = session_dict\n",
    "    \n",
    "    sub_onedrive_path = join(onedrive_path, sub)\n",
    "    sub_onedrive_path_task = join(onedrive_path, sub, 'synced_data', session_ID)\n",
    "    filename = [f for f in os.listdir(sub_onedrive_path_task) if (\n",
    "        f.endswith('.set') and f.startswith('SYNCHRONIZED_INTRACRANIAL'))]\n",
    "    \n",
    "    if not filename:\n",
    "        raise FileNotFoundError(f\"No .set file found in {sub_onedrive_path_task}\")\n",
    "\n",
    "    file = join(sub_onedrive_path_task, filename[0])\n",
    "\n",
    "    if not os.path.isfile(file):\n",
    "        raise FileNotFoundError(f\"File does not exist: {file}\")\n",
    "\n",
    "    print(f\"Loading file: {file}\")\n",
    "    #file = join(sub_onedrive_path_task, filename[0])\n",
    "    raw = read_raw(file, preload=True)\n",
    "    all_sub_session_dict[sub][condition]['CHANNELS'] = raw.ch_names\n",
    "\n",
    "    # Rename channels to be consistent across subjects:\n",
    "    new_channel_names = [\n",
    "        \"Left_STN\",\n",
    "        \"Right_STN\",\n",
    "        \"left_peak_STN\",\n",
    "        \"right_peak_STN\",\n",
    "        \"STIM_Left_STN\",\n",
    "        \"STIM_Right_STN\"\n",
    "    ]\n",
    "\n",
    "    # Get the existing channel names\n",
    "    old_channel_names = raw.ch_names\n",
    "\n",
    "    # Create a mapping from old to new names\n",
    "    rename_dict = {old: new for old, new in zip(old_channel_names, new_channel_names)}\n",
    "\n",
    "    # Rename the channels\n",
    "    raw.rename_channels(rename_dict)\n",
    "\n",
    "    all_sub_session_dict[sub][condition]['RENAMED_CHANNELS'] = raw.ch_names\n",
    "\n",
    "    # Filter between 1 and 80 Hz:\n",
    "    filtered_data = raw.copy().filter(l_freq=1, h_freq=80)\n",
    "\n",
    "    # Extract events and create epochs\n",
    "    # only keep lfp channels\n",
    "    filtered_data_lfp = filtered_data.copy().pick_channels([filtered_data.ch_names[0], filtered_data.ch_names[1]])\n",
    "\n",
    "    #epochs, filtered_event_dict = preprocessing.create_epochs(filtered_data_lfp, session_ID)\n",
    "\n",
    "    mSST_raw_behav_session_data_path = join(\n",
    "            onedrive_path, sub, \"raw_data\", 'BEHAVIOR', condition, 'mSST'\n",
    "            )\n",
    "    for filename in os.listdir(mSST_raw_behav_session_data_path):\n",
    "            if filename.endswith(\".csv\"):\n",
    "                fname = filename\n",
    "    filepath_behav = join(mSST_raw_behav_session_data_path, fname)\n",
    "    df = pd.read_csv(filepath_behav)\n",
    "\n",
    "    # return the index of the first row which is not filled by a Nan value:\n",
    "    start_task_index = df['blocks.thisRepN'].first_valid_index()\n",
    "    stop_task_index = df['blocks.thisRepN'].last_valid_index()\n",
    "    df_maintask = df.iloc[start_task_index:stop_task_index + 1] ### HERE MISTAKE OF INDEXING: CHECK IN OTHER SCRIPTS IF THIS IS ALSO WRONG!!!\n",
    "\n",
    "\n",
    "    # remove all useless columns to clean up dataframe\n",
    "    column_names = df_maintask.columns\n",
    "    columns_to_keep = [i for i in [\n",
    "        'blocks.thisN', 'trial_loop.thisN', 'trial_type', \n",
    "        'continue_signal_time', 'stop_signal_time', \n",
    "        'fixation_cross.started', 'go_rectangle.started',\n",
    "        'key_resp_experiment.keys', 'key_resp_experiment.corr', 'key_resp_experiment.rt',\n",
    "        'early_press_resp.keys', 'early_press_resp.rt', 'early_press_resp.corr',\n",
    "        'late_key_resp1.keys', 'late_key_resp1.rt', \n",
    "        'late_key_resp2.keys', 'late_key_resp2.rt'\n",
    "        ] if i in column_names]\n",
    "\n",
    "    mini_df_maintask = df_maintask[columns_to_keep]\n",
    "\n",
    "    # remove the trials with early presses, as in these trials the cues were not presented (for mSST)\n",
    "    early_presses = mini_df_maintask[mini_df_maintask['early_press_resp.corr'] == 1]\n",
    "    early_presses_trials = list(early_presses.index)\n",
    "    number_early_presses = len(early_presses_trials)\n",
    "\n",
    "    # remove trials with early presses from the dataframe:\n",
    "    df_maintask_copy = mini_df_maintask.drop(early_presses_trials).reset_index(drop=True)\n",
    "\n",
    "    # First generate global epochs (without taking into account success outcome)\n",
    "    # events and event_id used for epochs creation\n",
    "    events, event_id = mne.events_from_annotations(filtered_data_lfp)\n",
    "    epochs, filtered_event_dict = preprocessing.create_epochs(\n",
    "         filtered_data_lfp, \n",
    "         sub, \n",
    "         keys_to_keep = ['GC', 'GF', 'GO', 'GS', 'continue', 'stop'],\n",
    "         tmin = -3.5,\n",
    "         tmax = 3.5,\n",
    "         baseline=None\n",
    "         )\n",
    "    n_epochs = len(epochs)\n",
    "\n",
    "    # inverse mapping (event code -> label)\n",
    "    inv_event_id = {v: k for k, v in event_id.items()}\n",
    "\n",
    "    metadata = pd.DataFrame(index=np.arange(len(epochs)))\n",
    "    metadata[\"event\"] = [inv_event_id[e] for e in epochs.events[:, 2]]\n",
    "    metadata[\"sample\"] = epochs.events[:, 0]\n",
    "    metadata[\"event_timing\"] = epochs.events[:, 0] / raw.info['sfreq']  # in seconds\n",
    "    metadata[\"trial_type\"] = np.nan\n",
    "\n",
    "    # LFP -> behavioral naming mapping\n",
    "    mapping = {\n",
    "        \"GC\": \"go_continue_trial\",\n",
    "        \"GO\": \"go_trial\",\n",
    "        \"GF\": \"go_fast_trial\",\n",
    "        \"GS\": \"stop_trial\",\n",
    "    }\n",
    "\n",
    "    trial_mask = metadata[\"event\"].isin(mapping.keys())\n",
    "\n",
    "    assert trial_mask.sum() == len(df_maintask_copy), \\\n",
    "        f\"Mismatch: {trial_mask.sum()} LFP trials vs {len(df_maintask_copy)} behavioral trials\"\n",
    "\n",
    "    # fill directly from behavioral file\n",
    "    for col in df_maintask_copy.columns:\n",
    "        metadata.loc[trial_mask, col] = df_maintask_copy[col].values\n",
    "\n",
    "    for i in metadata.index:\n",
    "        if metadata.loc[i, \"event\"] == \"continue\":\n",
    "            # find the last GC before this\n",
    "            prev_idx = metadata.loc[:i-1][metadata[\"event\"] == \"GC\"].index[-1]\n",
    "            metadata.loc[i, df_maintask_copy.columns] = metadata.loc[prev_idx, df_maintask_copy.columns]\n",
    "\n",
    "        elif metadata.loc[i, \"event\"] == \"stop\":\n",
    "            # find the last GS before this\n",
    "            prev_idx = metadata.loc[:i-1][metadata[\"event\"] == \"GS\"].index[-1]\n",
    "            metadata.loc[i, df_maintask_copy.columns] = metadata.loc[prev_idx, df_maintask_copy.columns]\n",
    "\n",
    "    epochs.metadata = metadata\n",
    "\n",
    "    sub_dict_epochs[session_ID] = epochs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For each session, look at the epochs and remove 'bad' epochs, then save the cleaned file #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_id = 'sub023 DBS ON mSST'\n",
    "cleaned_epochs = sub_dict_epochs[subject_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Implementation of all the FASTER steps.\"\"\"\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import mne\n",
    "import numpy as np\n",
    "import scipy.signal\n",
    "from mne import pick_info\n",
    "from mne._fiff.pick import _picks_by_type\n",
    "from mne.preprocessing.bads import _find_outliers\n",
    "from mne.utils import logger\n",
    "from scipy.stats import kurtosis\n",
    "\n",
    "\n",
    "def _bad_mask_to_names(info, bad_mask):\n",
    "    \"\"\"Remap mask to ch names.\"\"\"\n",
    "    bad_idx = [np.where(m)[0] for m in bad_mask]\n",
    "    return [[info[\"ch_names\"][k] for k in epoch] for epoch in bad_idx]\n",
    "\n",
    "\n",
    "def _combine_indices(bads):\n",
    "    \"\"\"Summarize indices.\"\"\"\n",
    "    return list(set(v for val in bads.values() if len(val) > 0 for v in val))\n",
    "\n",
    "\n",
    "def hurst(x):\n",
    "    \"\"\"Estimate Hurst exponent on a timeseries.\n",
    "\n",
    "    The estimation is based on the second order discrete derivative.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : 1D numpy array\n",
    "        The timeseries to estimate the Hurst exponent for.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    h : float\n",
    "        The estimation of the Hurst exponent for the given timeseries.\n",
    "\n",
    "    \"\"\"\n",
    "    y = np.cumsum(np.diff(x, axis=1), axis=1)\n",
    "\n",
    "    b1 = [1, -2, 1]\n",
    "    b2 = [1, 0, -2, 0, 1]\n",
    "\n",
    "    # second order derivative\n",
    "    y1 = scipy.signal.lfilter(b1, 1, y, axis=1)\n",
    "    y1 = y1[:, len(b1) - 1 : -1]  # first values contain filter artifacts\n",
    "\n",
    "    # wider second order derivative\n",
    "    y2 = scipy.signal.lfilter(b2, 1, y, axis=1)\n",
    "    y2 = y2[:, len(b2) - 1 : -1]  # first values contain filter artifacts\n",
    "\n",
    "    s1 = np.mean(y1**2, axis=1)\n",
    "    s2 = np.mean(y2**2, axis=1)\n",
    "\n",
    "    return 0.5 * np.log2(s2 / s1)\n",
    "\n",
    "\n",
    "def _efficient_welch(data, sfreq):\n",
    "    \"\"\"Call scipy.signal.welch with parameters optimized for greatest speed.\n",
    "\n",
    "    Comes at the expense of precision. The window is set to ~10 seconds and windows are\n",
    "    non-overlapping.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : array, shape (..., n_samples)\n",
    "        The timeseries to estimate signal power for. The last dimension\n",
    "        is assumed to be time.\n",
    "    sfreq : float\n",
    "        The sample rate of the timeseries.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    fs : array of float\n",
    "        The frequencies for which the power spectra was calculated.\n",
    "    ps : array, shape (..., frequencies)\n",
    "        The power spectra for each timeseries.\n",
    "\n",
    "    \"\"\"\n",
    "    from scipy.signal import welch\n",
    "\n",
    "    nperseg = min(data.shape[-1], 2 ** int(np.log2(10 * sfreq) + 1))  # next power of 2\n",
    "\n",
    "    return welch(data, sfreq, nperseg=nperseg, noverlap=0, axis=-1)\n",
    "\n",
    "\n",
    "def _freqs_power(data, sfreq, freqs):\n",
    "    fs, ps = _efficient_welch(data, sfreq)\n",
    "    try:\n",
    "        return np.sum([ps[..., np.searchsorted(fs, f)] for f in freqs], axis=0)\n",
    "    except IndexError:\n",
    "        raise ValueError(\n",
    "            (\n",
    "                \"Insufficient sample rate to  estimate power at {} Hz for line \"\n",
    "                \"noise detection. Use the 'metrics' parameter to disable the \"\n",
    "                \"'line_noise' metric.\"\n",
    "            ).format(freqs)\n",
    "        )\n",
    "\n",
    "\n",
    "def _distance_correction(info, picks, x):\n",
    "    \"\"\"Remove the effect of distance to reference sensor.\n",
    "\n",
    "    Computes the distance of each sensor to the reference sensor. Then regresses the\n",
    "    effect of this distance out of the values in x.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    info : instance of Info\n",
    "        The measurement info. This should contain positions for all the sensors.\n",
    "    picks : list of int\n",
    "        Indices of the channels that correspond to the values in x.\n",
    "    x : list of float\n",
    "        Values to correct.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x_corr : list of float\n",
    "        values in x corrected for the distance to reference sensor.\n",
    "\n",
    "    \"\"\"\n",
    "    pos = np.array([info[\"chs\"][ch][\"loc\"][:3] for ch in picks])\n",
    "    ref_pos = np.array([info[\"chs\"][ch][\"loc\"][3:6] for ch in picks])\n",
    "\n",
    "    if np.any(np.all(pos == 0, axis=1)):\n",
    "        raise ValueError(\n",
    "            \"Cannot perform correction for distance to reference \"\n",
    "            \"sensor: not all selected channels have position \"\n",
    "            \"information.\"\n",
    "        )\n",
    "    if np.any(np.all(ref_pos == 0, axis=1)):\n",
    "        raise ValueError(\n",
    "            \"Cannot perform correction for distance to reference \"\n",
    "            \"sensor: the location of the reference sensor is not \"\n",
    "            \"specified for all selected channels.\"\n",
    "        )\n",
    "\n",
    "    # Compute angular distances to the reference sensor\n",
    "    pos /= np.linalg.norm(pos, axis=1)[:, np.newaxis]\n",
    "    ref_pos /= np.linalg.norm(ref_pos, axis=1)[:, np.newaxis]\n",
    "    angles = [np.arccos(np.dot(a, b)) for a, b in zip(pos, ref_pos)]\n",
    "\n",
    "    # Fit a quadratic curve to correct for the angular distance\n",
    "    fit = np.polyfit(angles, x, 2)\n",
    "    return x - np.polyval(fit, angles)\n",
    "\n",
    "\n",
    "def find_bad_channels(\n",
    "    epochs,\n",
    "    picks=None,\n",
    "    max_iter=1,\n",
    "    thres=3,\n",
    "    eeg_ref_corr=False,\n",
    "    use_metrics=None,\n",
    "    return_by_metric=False,\n",
    "):\n",
    "    \"\"\"Automatically find and mark bad channels.\n",
    "\n",
    "    Implements the first step of the FASTER algorithm.\n",
    "\n",
    "    This function attempts to automatically mark bad EEG channels by performing outlier\n",
    "    detection. It operated on epoched data, to make sure only relevant data is analyzed.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    epochs : Instance of Epochs\n",
    "        The epochs for which bad channels need to be marked\n",
    "    picks : list of int | None\n",
    "        Channels to operate on. Defaults to EEG channels.\n",
    "    thres : float\n",
    "        The threshold value, in standard deviations, to apply. A channel crossing this\n",
    "        threshold value is marked as bad. Defaults to 3.\n",
    "    max_iter : int\n",
    "        The maximum number of iterations performed during outlier detection\n",
    "        (defaults to 1, as in the original FASTER paper).\n",
    "    eeg_ref_corr : bool\n",
    "        If the EEG data has been referenced using a single electrode setting this\n",
    "        parameter to True will enable a correction factor for the distance of each\n",
    "        electrode to the reference. If an average reference is applied, or the mean of\n",
    "        multiple reference electrodes, set this parameter to False. Defaults to False,\n",
    "        which disables the correction.\n",
    "    use_metrics : list of str\n",
    "        List of metrics to use. Can be any combination of:\n",
    "            'variance', 'correlation', 'hurst', 'kurtosis', 'line_noise'\n",
    "        Defaults to all of them.\n",
    "    return_by_metric : bool\n",
    "        Whether to return the bad channels as a flat list (False, default) or as a\n",
    "        dictionary with the names of the used metrics as keys and the bad channels found\n",
    "        by this metric as values.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bads : list of str\n",
    "        The names of the bad EEG channels.\n",
    "\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        \"variance\": lambda x: np.var(x, axis=1),\n",
    "        \"correlation\": lambda x: np.nanmean(\n",
    "            np.ma.masked_array(np.corrcoef(x), np.identity(len(x), dtype=bool)), axis=0\n",
    "        ),\n",
    "        \"hurst\": lambda x: hurst(x),\n",
    "        \"kurtosis\": lambda x: kurtosis(x, axis=1),\n",
    "        \"line_noise\": lambda x: _freqs_power(x, epochs.info[\"sfreq\"], [50, 60]),\n",
    "    }\n",
    "\n",
    "    if picks is None:\n",
    "        picks = mne.pick_types(epochs.info, meg=False, eeg=True, exclude=[])\n",
    "    if use_metrics is None:\n",
    "        use_metrics = metrics.keys()\n",
    "\n",
    "    # Concatenate epochs in time\n",
    "    data = epochs.get_data(copy=False)[:, picks]\n",
    "    data = data.transpose(1, 0, 2).reshape(data.shape[1], -1)\n",
    "\n",
    "    # Find bad channels\n",
    "    bads = defaultdict(list)\n",
    "    info = pick_info(epochs.info, picks, copy=True)\n",
    "    for ch_type, chs in _picks_by_type(info):\n",
    "        logger.info(\"Bad channel detection on %s channels:\" % ch_type.upper())\n",
    "        for metric in use_metrics:\n",
    "            scores = metrics[metric](data[chs])\n",
    "            if eeg_ref_corr:\n",
    "                scores = _distance_correction(epochs.info, picks, scores)\n",
    "            bad_channels = [\n",
    "                epochs.ch_names[picks[chs[i]]]\n",
    "                for i in _find_outliers(scores, thres, max_iter)\n",
    "            ]\n",
    "            logger.info(\"\\tBad by %s: %s\" % (metric, bad_channels))\n",
    "            bads[metric].append(bad_channels)\n",
    "\n",
    "    bads = dict((k, np.concatenate(v).tolist()) for k, v in bads.items())\n",
    "\n",
    "    if return_by_metric:\n",
    "        return bads\n",
    "    else:\n",
    "        return _combine_indices(bads)\n",
    "\n",
    "\n",
    "def _deviation(data):\n",
    "    \"\"\"Compute the deviation from mean for each channel in a set of epochs.\n",
    "\n",
    "    This is not implemented as a lambda function, because the channel means should be\n",
    "    cached during the computation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : 3D numpy array\n",
    "        The epochs (#epochs x #channels x #samples).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dev : list of float\n",
    "        For each epoch, the mean deviation of the channels.\n",
    "\n",
    "    \"\"\"\n",
    "    ch_mean = np.mean(data, axis=2)\n",
    "    return ch_mean - np.mean(ch_mean, axis=0)\n",
    "\n",
    "\n",
    "def find_bad_epochs(\n",
    "    epochs, picks=None, thres=3, max_iter=1, use_metrics=None, return_by_metric=False\n",
    "):\n",
    "    \"\"\"Automatically find and mark bad epochs.\n",
    "\n",
    "    Implements the second step of the FASTER algorithm.\n",
    "\n",
    "    This function attempts to automatically mark bad epochs by performing outlier\n",
    "    detection.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    epochs : Instance of Epochs\n",
    "        The epochs to analyze.\n",
    "    picks : list of int | None\n",
    "        Channels to operate on. Defaults to EEG channels.\n",
    "    thres : float\n",
    "        The threshold value, in standard deviations, to apply. An epoch\n",
    "        crossing this threshold value is marked as bad. Defaults to 3.\n",
    "    max_iter : int\n",
    "        The maximum number of iterations performed during outlier detection\n",
    "        (defaults to 1, as in the original FASTER paper).\n",
    "    use_metrics : list of str\n",
    "        List of metrics to use. Can be any combination of:\n",
    "            'amplitude', 'variance', 'deviation'\n",
    "        Defaults to all of them.\n",
    "    return_by_metric : bool\n",
    "        Whether to return the bad channels as a flat list (False, default) or as a\n",
    "        dictionary with the names of the used metrics as keys and the bad channels found\n",
    "        by this metric as values.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bads : list of int\n",
    "        The indices of the bad epochs.\n",
    "\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        \"amplitude\": lambda x: np.mean(np.ptp(x, axis=2), axis=1),\n",
    "        \"deviation\": lambda x: np.mean(_deviation(x), axis=1),\n",
    "        \"variance\": lambda x: np.mean(np.var(x, axis=2), axis=1),\n",
    "    }\n",
    "\n",
    "    if picks is None:\n",
    "        picks = mne.pick_types(epochs.info, meg=False, eeg=True, exclude=\"bads\")\n",
    "    if use_metrics is None:\n",
    "        use_metrics = metrics.keys()\n",
    "\n",
    "    info = pick_info(epochs.info, picks, copy=True)\n",
    "    data = epochs.get_data(copy=True)[:, picks]\n",
    "\n",
    "    bads = defaultdict(list)\n",
    "    for ch_type, chs in _picks_by_type(info):\n",
    "        #logger.info(\"Bad epoch detection on %s channels:\" % ch_type.upper())\n",
    "        for metric in use_metrics:\n",
    "            scores = metrics[metric](data[:, chs])\n",
    "            bad_epochs = _find_outliers(scores, thres, max_iter)\n",
    "            #logger.info(\"\\tBad by %s: %s\" % (metric, bad_epochs))\n",
    "            bads[metric].append(bad_epochs)\n",
    "\n",
    "    bads = dict((k, np.concatenate(v).tolist()) for k, v in bads.items())\n",
    "    if return_by_metric:\n",
    "        return bads\n",
    "    else:\n",
    "        return _combine_indices(bads)\n",
    "\n",
    "\n",
    "def _power_gradient(data, sfreq, prange):\n",
    "    \"\"\"Estimate the gradient of the power spectrum at upper frequencies.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : array, shape (n_components, n_samples)\n",
    "        The timeseries to estimate signal power for. The last dimension is presumed to\n",
    "        be time.\n",
    "    sfreq : float\n",
    "        The sample rate of the timeseries.\n",
    "    prange : pair of floats\n",
    "        The (lower, upper) frequency limits of the power spectrum to use. In the FASTER\n",
    "        paper, they set these to the passband of the lowpass filter.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    grad : array of float\n",
    "        The gradients of the timeseries.\n",
    "\n",
    "    \"\"\"\n",
    "    fs, ps = _efficient_welch(data, sfreq)\n",
    "\n",
    "    # Limit power spectrum to selected frequencies\n",
    "    start, stop = (np.searchsorted(fs, p) for p in prange)\n",
    "    if start >= ps.shape[1]:\n",
    "        raise ValueError(\n",
    "            (\n",
    "                \"Sample rate insufficient to estimate {} Hz power. \"\n",
    "                \"Use the 'power_gradient_range' parameter to tweak \"\n",
    "                \"the tested frequencies for this metric or use the \"\n",
    "                \"'metrics' parameter to disable the \"\n",
    "                \"'power_gradient' metric.\"\n",
    "            ).format(prange[0])\n",
    "        )\n",
    "    ps = ps[:, start:stop]\n",
    "\n",
    "    # Compute mean gradients\n",
    "    return np.mean(np.diff(ps), axis=1)\n",
    "\n",
    "\n",
    "def find_bad_components(\n",
    "    ica,\n",
    "    epochs,\n",
    "    thres=3,\n",
    "    max_iter=1,\n",
    "    use_metrics=None,\n",
    "    prange=None,\n",
    "    return_by_metric=False,\n",
    "):\n",
    "    \"\"\"Perform the third step of the FASTER algorithm.\n",
    "\n",
    "    This function attempts to automatically mark bad ICA components by performing\n",
    "    outlier detection.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ica : Instance of ICA\n",
    "        The ICA operator, already fitted to the supplied Epochs object.\n",
    "    epochs : Instance of Epochs\n",
    "        The untransformed epochs to analyze.\n",
    "    thres : float\n",
    "        The threshold value, in standard deviations, to apply. A component crossing this\n",
    "        threshold value is marked as bad. Defaults to 3.\n",
    "    max_iter : int\n",
    "        The maximum number of iterations performed during outlier detection\n",
    "        (defaults to 1, as in the original FASTER paper).\n",
    "    use_metrics : list of str\n",
    "        List of metrics to use. Can be any combination of:\n",
    "            'eog_correlation', 'kurtosis', 'power_gradient', 'hurst',\n",
    "            'median_gradient'\n",
    "        Defaults to all of them.\n",
    "    prange : None | pair of floats\n",
    "        The (lower, upper) frequency limits of the power spectrum to use for the power\n",
    "        gradient computation. In the FASTER paper, they set these to the passband of the\n",
    "        highpass and lowpass filter. If None, defaults to the 'highpass' and 'lowpass'\n",
    "        filter settings in ica.info.\n",
    "    return_by_metric : bool\n",
    "        Whether to return the bad channels as a flat list (False, default) or as a\n",
    "        dictionary with the names of the used metrics as keys and the bad channels found\n",
    "        by this metric as values.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bads : list of int\n",
    "        The indices of the bad components.\n",
    "\n",
    "    See Also\n",
    "    --------\n",
    "    ICA.find_bads_ecg\n",
    "    ICA.find_bads_eog\n",
    "\n",
    "    \"\"\"\n",
    "    source_data = ica.get_sources(epochs).get_data(copy=False).transpose(1, 0, 2)\n",
    "    source_data = source_data.reshape(source_data.shape[0], -1)\n",
    "\n",
    "    if prange is None:\n",
    "        prange = (ica.info[\"highpass\"], ica.info[\"lowpass\"])\n",
    "    if len(prange) != 2:\n",
    "        raise ValueError(\"prange must be a pair of floats\")\n",
    "\n",
    "    metrics = {\n",
    "        \"eog_correlation\": lambda x: x.find_bads_eog(epochs)[1],\n",
    "        \"kurtosis\": lambda x: kurtosis(\n",
    "            np.dot(x.mixing_matrix_.T, x.pca_components_[: x.n_components_]), axis=1\n",
    "        ),\n",
    "        \"power_gradient\": lambda x: _power_gradient(\n",
    "            source_data, ica.info[\"sfreq\"], prange\n",
    "        ),\n",
    "        \"hurst\": lambda x: hurst(source_data),\n",
    "        \"median_gradient\": lambda x: np.median(np.abs(np.diff(source_data)), axis=1),\n",
    "        \"line_noise\": lambda x: _freqs_power(\n",
    "            source_data, epochs.info[\"sfreq\"], [50, 60]\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    if use_metrics is None:\n",
    "        use_metrics = metrics.keys()\n",
    "\n",
    "    bads = defaultdict(list)\n",
    "    for metric in use_metrics:\n",
    "        scores = np.atleast_2d(metrics[metric](ica))\n",
    "        for s in scores:\n",
    "            bad_comps = _find_outliers(s, thres, max_iter)\n",
    "            logger.info(\"Bad by %s:\\n\\t%s\" % (metric, bad_comps))\n",
    "            bads[metric].append(bad_comps)\n",
    "\n",
    "    bads = dict((k, np.concatenate(v).tolist()) for k, v in bads.items())\n",
    "    if return_by_metric:\n",
    "        return bads\n",
    "    else:\n",
    "        return _combine_indices(bads)\n",
    "\n",
    "\n",
    "def find_bad_channels_in_epochs(\n",
    "    epochs,\n",
    "    picks=None,\n",
    "    thres=3,\n",
    "    max_iter=1,\n",
    "    eeg_ref_corr=False,\n",
    "    use_metrics=None,\n",
    "    return_by_metric=False,\n",
    "):\n",
    "    \"\"\"Perform the fourth step of the FASTER algorithm.\n",
    "\n",
    "    This function attempts to automatically mark bad channels in each epochs by\n",
    "    performing outlier detection.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    epochs : Instance of Epochs\n",
    "        The epochs to analyze.\n",
    "    picks : list of int | None\n",
    "        Channels to operate on. Defaults to EEG channels.\n",
    "    thres : float\n",
    "        The threshold value, in standard deviations, to apply. An epoch crossing this\n",
    "        threshold value is marked as bad. Defaults to 3.\n",
    "    max_iter : int\n",
    "        The maximum number of iterations performed during outlier detection\n",
    "        (defaults to 1, as in the original FASTER paper).\n",
    "    eeg_ref_corr : bool\n",
    "        If the EEG data has been referenced using a single electrode setting this\n",
    "        parameter to True will enable a correction factor for the distance of each\n",
    "        electrode to the reference. If an average reference is applied, or the mean of\n",
    "        multiple reference electrodes, set this parameter to False. Defaults to False,\n",
    "        which disables the correction.\n",
    "    use_metrics : list of str\n",
    "        List of metrics to use. Can be any combination of:\n",
    "            'amplitude', 'variance', 'deviation', 'median_gradient'\n",
    "        Defaults to all of them.\n",
    "    return_by_metric : bool\n",
    "        Whether to return the bad channels as a flat list (False, default) or as a\n",
    "        dictionary with the names of the used metrics as keys and the bad channels found\n",
    "        by this metric as values.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bads : list of lists of int\n",
    "        For each epoch, the indices of the bad channels.\n",
    "\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        \"amplitude\": lambda x: np.ptp(x, axis=2),\n",
    "        \"deviation\": lambda x: _deviation(x),\n",
    "        \"variance\": lambda x: np.var(x, axis=2),\n",
    "        \"median_gradient\": lambda x: np.median(np.abs(np.diff(x)), axis=2),\n",
    "        \"line_noise\": lambda x: _freqs_power(x, epochs.info[\"sfreq\"], [50, 60]),\n",
    "    }\n",
    "\n",
    "    if picks is None:\n",
    "        picks = mne.pick_types(epochs.info, meg=False, eeg=True, exclude=\"bads\")\n",
    "    if use_metrics is None:\n",
    "        use_metrics = metrics.keys()\n",
    "\n",
    "    info = pick_info(epochs.info, picks, copy=True)\n",
    "    data = epochs.get_data(copy=False)[:, picks]\n",
    "    bads = dict((m, np.zeros((len(data), len(picks)), dtype=bool)) for m in metrics)\n",
    "    for ch_type, chs in _picks_by_type(info):\n",
    "        ch_names = [info[\"ch_names\"][k] for k in chs]\n",
    "        chs = np.array(chs)\n",
    "        for metric in use_metrics:\n",
    "            logger.info(\n",
    "                \"Bad channel-in-epoch detection on %s channels:\" % ch_type.upper()\n",
    "            )\n",
    "            s_epochs = metrics[metric](data[:, chs])\n",
    "            for i_epochs, scores in enumerate(s_epochs):\n",
    "                if eeg_ref_corr:\n",
    "                    scores = _distance_correction(epochs.info, picks, scores)\n",
    "                outliers = _find_outliers(scores, thres, max_iter)\n",
    "                if len(outliers) > 0:\n",
    "                    bad_segment = [ch_names[k] for k in outliers]\n",
    "                    logger.info(\n",
    "                        \"Epoch %d, Bad by %s:\\n\\t%s\" % (i_epochs, metric, bad_segment)\n",
    "                    )\n",
    "                    bads[metric][i_epochs, chs[outliers]] = True\n",
    "\n",
    "    info = pick_info(epochs.info, picks, copy=True)\n",
    "    if return_by_metric:\n",
    "        bads = dict((m, _bad_mask_to_names(info, v)) for m, v in bads.items())\n",
    "    else:\n",
    "        bads = np.sum(list(bads.values()), axis=0).astype(bool)\n",
    "        bads = _bad_mask_to_names(info, bads)\n",
    "\n",
    "    return bads\n",
    "\n",
    "\n",
    "def run_faster(epochs, thres=3, copy=True):\n",
    "    \"\"\"Run the entire FASTER pipeline on the data.\"\"\"\n",
    "    if copy:\n",
    "        epochs = epochs.copy()\n",
    "\n",
    "    # Step one\n",
    "    logger.info(\"Step 1: mark bad channels\")\n",
    "    epochs.info[\"bads\"] += find_bad_channels(epochs, thres=5)\n",
    "\n",
    "    # Step two\n",
    "    logger.info(\"Step 2: mark bad epochs\")\n",
    "    bad_epochs = find_bad_epochs(epochs, thres=thres)\n",
    "    good_epochs = list(set(range(len(epochs))).difference(set(bad_epochs)))\n",
    "    epochs = epochs[good_epochs]\n",
    "\n",
    "    # Step three (using the build-in MNE functionality for this)\n",
    "    logger.info(\"Step 3: mark bad ICA components\")\n",
    "    picks = mne.pick_types(epochs.info, meg=False, eeg=True, eog=True, exclude=\"bads\")\n",
    "    ica = mne.preprocessing.ICA(len(picks)).fit(epochs, picks=picks)\n",
    "    ica.exclude = find_bad_components(ica, epochs, thres=thres)\n",
    "    ica.apply(epochs)\n",
    "    epochs.apply_baseline(epochs.baseline)\n",
    "\n",
    "    # Step four\n",
    "    logger.info(\"Step 4: mark bad channels for each epoch\")\n",
    "    bad_channels_per_epoch = find_bad_channels_in_epochs(epochs, thres=thres)\n",
    "    for i, b in enumerate(bad_channels_per_epoch):\n",
    "        if len(b) > 0:\n",
    "            epoch = epochs[i]\n",
    "            epoch.info[\"bads\"] += b\n",
    "            epoch.interpolate_bads()\n",
    "            epochs._data[i, :, :] = epoch._data[0, :, :]\n",
    "\n",
    "    # Now that the data is clean, apply average reference\n",
    "    epochs.set_eeg_reference(\"average\")\n",
    "\n",
    "    # That's all for now\n",
    "    return epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mne._fiff.pick import _picks_by_type\n",
    "from scipy.stats import zscore\n",
    "\n",
    "epochs = cleaned_epochs.copy()\n",
    "\n",
    "metrics = {\n",
    "    \"amplitude\": lambda x: np.mean(np.ptp(x, axis=2), axis=1),\n",
    "    \"deviation\": lambda x: np.mean(_deviation(x), axis=1),\n",
    "    \"variance\": lambda x: np.mean(np.var(x, axis=2), axis=1),\n",
    "}\n",
    "thresh = 3\n",
    "max_iter = 1\n",
    "tail=0\n",
    "picks = mne.pick_types(epochs.info, meg=False, eeg=True, exclude=\"bads\")\n",
    "use_metrics = metrics.keys()\n",
    "\n",
    "info = pick_info(epochs.info, picks, copy=True)\n",
    "data = epochs.get_data(copy=True)[:, picks]\n",
    "\n",
    "bads={}\n",
    "all_bads = []\n",
    "bads = defaultdict(list)\n",
    "for ch_type, chs in _picks_by_type(info):\n",
    "    #logger.info(\"Bad epoch detection on %s channels:\" % ch_type.upper())\n",
    "    for metric in use_metrics:\n",
    "        scores = metrics[metric](data[:, chs])\n",
    "\n",
    "        ############################################\"\"\n",
    "        my_mask = np.zeros(len(scores), dtype=bool)\n",
    "        for _ in range(max_iter):\n",
    "            scores = np.ma.masked_array(scores, my_mask)\n",
    "            if tail == 0:\n",
    "                this_z = np.abs(zscore(scores))\n",
    "            elif tail == 1:\n",
    "                this_z = zscore(scores)\n",
    "            elif tail == -1:\n",
    "                this_z = -zscore(scores)\n",
    "            else:\n",
    "                raise ValueError(f\"Tail parameter {tail} not recognised.\")\n",
    "            local_bad = this_z > thresh\n",
    "            my_mask = np.max([my_mask, local_bad], 0)\n",
    "            if not np.any(local_bad):\n",
    "                break\n",
    "\n",
    "        bad_epochs = np.where(my_mask)[0]\n",
    "        ##############################################\"\"\n",
    "        \n",
    "        #bad_epochs = _find_outliers(scores, thres, max_iter)\n",
    "        #logger.info(\"\\tBad by %s: %s\" % (metric, bad_epochs))\n",
    "        bads[metric].append(bad_epochs)\n",
    "        all_bads.append(bad_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_bads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: mark bad epochs\n",
    "bad_epochs = find_bad_epochs(cleaned_epochs.copy(), return_by_metric=False)\n",
    "bad_epochs.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "fig = cleaned_epochs.plot(n_epochs=4, events=True, block=True)\n",
    "plt.close(fig)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "# cleaned_epochs.plot(n_epochs=10, n_channels = len(cleaned_epochs.ch_names), events=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nan_bads = [252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272 ,273, 274, 275, 276, 277, 278, 279, 280]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned_epochs.drop(nan_bads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "cleaned_epochs.plot(n_epochs=4, n_channels = len(cleaned_epochs.ch_names), events=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df = pd.DataFrame(cleaned_epochs.metadata)\n",
    "# save both to csv (easier for later python import), and xlsx (easier to read in excel)\n",
    "metadata_df.to_csv(os.path.join(saving_path, f\"{subject_id}_cleaned-long-epo_metadata.csv\"), index=True)\n",
    "metadata_df.to_excel(os.path.join(saving_path, f\"{subject_id}_cleaned-long-epo_metadata.xlsx\"), index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_epoch = os.path.join(saving_path, f\"{subject_id}_cleaned-long-epo.fif\")\n",
    "cleaned_epochs.save(file_epoch, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#epoch_reload = mne.read_epochs(os.path.join(saving_path, f\"sub011 DBS OFF mSST_cleaned-long-epo.fif\"), preload=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cropped_epochs = cleaned_epochs.copy().crop(tmin=-0.5, tmax=1.5)\n",
    "# file_cropped_epoch = os.path.join(saving_path, f\"{subject_id}_cleaned-short-epo.fif\")\n",
    "# cropped_epochs.save(file_cropped_epoch, overwrite=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ephy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
