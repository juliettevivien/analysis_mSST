{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c9f2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "from mne.io import read_raw\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#import matplotlib.colors as mcolors\n",
    "from os.path import join\n",
    "import os\n",
    "import pandas as pd\n",
    "from scipy.signal import hilbert\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "\n",
    "from functions import ephy_plotting, preprocessing, analysis, io, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270d589d",
   "metadata": {},
   "outputs": [],
   "source": [
    "working_path = os.path.dirname(os.getcwd())\n",
    "results_path = join(working_path, \"results\")\n",
    "behav_results_saving_path = join(results_path, \"behav_results\")\n",
    "# read the json file containing the included and excluded subjects, based on the behavioral results\n",
    "included_excluded_file = join(behav_results_saving_path, 'final_included_subjects.json')\n",
    "with open(included_excluded_file, 'r') as file:\n",
    "    included_subjects = json.load(file)\n",
    "\n",
    "# keep only subjects starting with \"sub\":\n",
    "included_subjects = [subj for subj in included_subjects if subj.startswith('sub')]\n",
    "print(f'Included_subjects: {included_subjects}')\n",
    "onedrive_path = utils._get_onedrive_path()\n",
    "\n",
    "#  Set saving path\n",
    "saving_path_group = join(results_path, 'group_level', 'lfp_perc_sig_change', 'morlet_low_freq')  \n",
    "os.makedirs(saving_path_group, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "saving_path_on_off = join(results_path, 'ON_vs_OFF', 'morlet_low_freq')\n",
    "os.makedirs(saving_path_on_off, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c18fd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare variables and dictionaries for storing results\n",
    "# Dictionary to store subject epochs in\n",
    "sub_dict_epochs_subsets = {}  #  Stores the epochs for each condition and for each subject/session\n",
    "sub_dict_lm_GO = {}  #  Stores the epochs for lm_GO trials for each subject/session\n",
    "sub_dict_RT = {}  #  Stores the mean reaction time for each trial type\n",
    "sub_dict_stats = {}  #  Stores behavioral stats for each subject/session\n",
    "\n",
    "cluster_results_dict = {}\n",
    "cluster_results_dict = defaultdict(dict)  # Each missing key gets an empty dictionary\n",
    "cluster_results_dict['All subjects'] = included_subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d863985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all data for all included subjects\n",
    "data = io.load_behav_data(included_subjects, onedrive_path)\n",
    "\n",
    "# Compute statistics for each loaded subject\n",
    "stats = {}\n",
    "stats = utils.extract_stats(data)\n",
    "# If no file was found, create a new JSON file\n",
    "filename = \"stats.json\"\n",
    "file_path = os.path.join(results_path, filename)\n",
    "#if not os.path.isfile(file_path):\n",
    "#    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "#            json.dump({}, file, indent=4)\n",
    "\n",
    "# Save the updated or new JSON file\n",
    "with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(stats, file, indent=4)\n",
    "\n",
    "# remove sub023\n",
    "included_subjects.remove('sub023 DBS ON mSST')\n",
    "included_subjects.remove('sub023 DBS OFF mSST')\n",
    "included_subjects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4ce3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all data for all included subjects\n",
    "data = io.load_behav_data(included_subjects, onedrive_path)\n",
    "\n",
    "# Compute statistics for each loaded subject\n",
    "stats = {}\n",
    "stats = utils.extract_stats(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1889c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a loop through subjects\n",
    "for session_ID in included_subjects:\n",
    "    print(f\"Now processing {session_ID}\")\n",
    "    session_dict = {}\n",
    "    sub = session_ID[:6]\n",
    "    subject_ID = session_ID.split(' ') [0]\n",
    "    condition = session_ID.split(' ') [1] + ' ' + session_ID.split(' ') [2]\n",
    "    sub_onedrive_path = join(onedrive_path, subject_ID)\n",
    "    sub_onedrive_path_task = join(onedrive_path, subject_ID, 'synced_data', session_ID)\n",
    "    filename = [f for f in os.listdir(sub_onedrive_path_task) if (\n",
    "        f.endswith('.set') and f.startswith('SYNCHRONIZED_INTRACRANIAL'))]\n",
    "    \n",
    "    if not filename:\n",
    "        raise FileNotFoundError(f\"No .set file found in {sub_onedrive_path_task}\")\n",
    "\n",
    "    file = join(sub_onedrive_path_task, filename[0])\n",
    "\n",
    "    if not os.path.isfile(file):\n",
    "        raise FileNotFoundError(f\"File does not exist: {file}\")\n",
    "\n",
    "    print(f\"Loading file: {file}\")\n",
    "    #file = join(sub_onedrive_path_task, filename[0])\n",
    "    raw = read_raw(file, preload=True)\n",
    "\n",
    "    saving_path_single = join(results_path, 'single_sub', f'{sub} mSST','freq_response') \n",
    "    os.makedirs(saving_path_single, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "\n",
    "    session_dict['CHANNELS'] = raw.ch_names\n",
    "\n",
    "    # Rename channels to be consistent across subjects:\n",
    "    new_channel_names = [\n",
    "        \"Left_STN\",\n",
    "        \"Right_STN\",\n",
    "        \"left_peak_STN\",\n",
    "        \"right_peak_STN\",\n",
    "        \"STIM_Left_STN\",\n",
    "        \"STIM_Right_STN\"\n",
    "    ]\n",
    "\n",
    "    # Get the existing channel names\n",
    "    old_channel_names = raw.ch_names\n",
    "\n",
    "    # Create a mapping from old to new names\n",
    "    rename_dict = {old: new for old, new in zip(old_channel_names, new_channel_names)}\n",
    "\n",
    "    # Rename the channels\n",
    "    raw.rename_channels(rename_dict)\n",
    "\n",
    "    session_dict['RENAMED_CHANNELS'] = raw.ch_names\n",
    "\n",
    "    # Filter between 1 and 95 Hz:\n",
    "    filtered_data = raw.copy().filter(l_freq=1, h_freq=95)\n",
    "\n",
    "    # Extract events and create epochs\n",
    "    # only keep lfp channels\n",
    "    filtered_data_lfp = filtered_data.copy().pick_channels([filtered_data.ch_names[0], filtered_data.ch_names[1]])\n",
    "\n",
    "    mSST_raw_behav_session_data_path = join(\n",
    "            onedrive_path, subject_ID, \"raw_data\", 'BEHAVIOR', condition, 'mSST'\n",
    "            )\n",
    "    for filename in os.listdir(mSST_raw_behav_session_data_path):\n",
    "            if filename.endswith(\".csv\"):\n",
    "                fname = filename\n",
    "    filepath_behav = join(mSST_raw_behav_session_data_path, fname)\n",
    "    df = pd.read_csv(filepath_behav)\n",
    "\n",
    "    # return the index of the first row which is not filled by a Nan value:\n",
    "    start_task_index = df['blocks.thisRepN'].first_valid_index()\n",
    "    # Crop dataframe in 2 parts: before and after the task:\n",
    "    #df_training = df.iloc[:start_task_index]\n",
    "    df_maintask = df.iloc[start_task_index:-1]\n",
    "\n",
    "    # remove the trials with early presses, as in these trials the cues were not presented\n",
    "    early_presses = df_maintask[df_maintask['early_press_resp.corr'] == 1]\n",
    "    early_presses_trials = list(early_presses.index)\n",
    "    number_early_presses = len(early_presses_trials)\n",
    "\n",
    "    # remove trials with early presses from the dataframe:\n",
    "    df_maintask_copy = df_maintask.drop(early_presses_trials)        \n",
    "\n",
    "    # Filter the channels in specific frequency bands e.g. theta\n",
    "    theta = [13, 20]\n",
    "    filtered_theta = filtered_data_lfp.copy().filter(l_freq=theta[0], h_freq=theta[1])\n",
    "    # Initialize an empty Raw object to store the Î²A signals\n",
    "    band_data = raw.copy()\n",
    "    # Iterate over each channel\n",
    "    channels_to_process = raw.ch_names[:2]\n",
    "    for channel in channels_to_process:\n",
    "        print(f\"Processing channel: {channel}\")\n",
    "\n",
    "        # Create a copy of the original data for filtering\n",
    "        single_channel_data = raw.copy().pick([channel])  # Use the new `pick()` method\n",
    "\n",
    "        # Initialize an array to store envelopes for each band\n",
    "        envelopes = []\n",
    "\n",
    "        low = theta[0]\n",
    "        high = theta[1]\n",
    "        # Apply bandpass filtering\n",
    "        filtered = mne.filter.filter_data(\n",
    "            single_channel_data.get_data().flatten(),\n",
    "            sfreq=raw.info['sfreq'],\n",
    "            l_freq=low,\n",
    "            h_freq=high,\n",
    "            method='fir',  # Zero-phase FIR filter\n",
    "            verbose=False,\n",
    "            fir_design = 'firwin',\n",
    "            l_trans_bandwidth=0.5,\n",
    "            h_trans_bandwidth=0.5\n",
    "        )\n",
    "\n",
    "        # Compute the analytic signal (Hilbert transform)\n",
    "        analytic_signal = hilbert(filtered)\n",
    "        envelope = np.abs(analytic_signal)\n",
    "\n",
    "        # Normalize the envelope\n",
    "        #normalized_envelope = (envelope - np.nanmean(envelope)) * 100\n",
    "        normalized_envelope = envelope\n",
    "        envelopes.append(normalized_envelope)\n",
    "\n",
    "        #plt.plot(single_channel_data.get_data())\n",
    "        #plt.plot(filtered, color=\"grey\")\n",
    "        #plt.plot(envelope, color=\"black\")\n",
    "        #plt.plot(normalized_envelope, color=\"red\")\n",
    "        #plt.legend()\n",
    "        #plt.show(block=True)\n",
    "\n",
    "        # Average the envelopes across bands\n",
    "        #freq_resp = np.nanmean(envelopes, axis=0)\n",
    "        #plt.plot(freq_response)\n",
    "\n",
    "        # Replace the corresponding channel's data in `band_data`\n",
    "        band_data._data[raw.ch_names.index(channel), :] = normalized_envelope\n",
    "\n",
    "\n",
    "    # create epochs for the theta band\n",
    "    epochs, filtered_event_dict = preprocessing.create_epochs(band_data, session_ID)\n",
    "    \n",
    "    # Filter successful and unsuccessful trials:\n",
    "    (epochs_subsets, epochs_lm, mean_RT_dict) = preprocessing.create_epochs_subsets_from_behav(\n",
    "            df_maintask_copy, \n",
    "            epochs, \n",
    "            filtered_event_dict\n",
    "            )\n",
    "    \n",
    "    sub_dict_epochs_subsets[session_ID] = epochs_subsets\n",
    "    sub_dict_lm_GO[session_ID] = epochs_lm\n",
    "    sub_dict_RT[session_ID] = mean_RT_dict\n",
    "    sub_dict_stats[session_ID] = stats[session_ID]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f530708",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_dict_epochs_subsets['sub006 DBS ON mSST']['GS_successful'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba1fa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "sub_dict_epochs_subsets['sub006 DBS ON mSST']['GO_successful'].plot_image(picks = (raw.ch_names[0]), combine=\"mean\")\n",
    "sub_dict_epochs_subsets['sub006 DBS ON mSST']['GO_successful'].plot_image(picks = (raw.ch_names[1]), combine=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c39f15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RT = stats['sub011 DBS OFF mSST']['go_trial RTs (ms)']\n",
    "# RT_df = pd.DataFrame({'RT': RT, 'index': range(len(RT))})\n",
    "# RT_sec = RT_df['RT'].values / 1000  # Convert to seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f01144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sort the dataframe by RT\n",
    "# RT_df = RT_df.sort_values(by='RT')\n",
    "# # add a column with the rank of the RT\n",
    "# RT_df['rank'] = RT_df['RT'].rank(method='min')\n",
    "# # sort the RT_df by index\n",
    "# RT_df = RT_df.sort_values(by='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e042e253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # extract the rank values and store them as integers\n",
    "# rank_values = RT_df['rank'].values\n",
    "# rank_values = rank_values - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2208494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rank_values.tolist()\n",
    "# # Convert rank values to integers\n",
    "# rank_values = rank_values.astype(int)\n",
    "# rank_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b680c749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Epochs object\n",
    "epochs = sub_dict_epochs_subsets['sub015 DBS ON mSST']['GO_successful']\n",
    "\n",
    "# Get the RTs (in seconds) for these epochs\n",
    "RT_sec = np.array(stats['sub015 DBS ON mSST']['go_trial RTs (ms)']) / 1000  # Assuming this is where RTs are stored\n",
    "\n",
    "# # Rank/order the RTs (ascending order)\n",
    "# rank_order = np.argsort(RT_sec)\n",
    "\n",
    "# # Sort RTs to match the rank order\n",
    "# RT_sec_sorted = np.array(RT_sec)[rank_order]\n",
    "\n",
    "# # Now plot with consistent ordering\n",
    "# epochs.plot_image(\n",
    "#     picks=raw.ch_names[0],\n",
    "#     combine=\"mean\",\n",
    "#     order=rank_order,\n",
    "#     overlay_times=RT_sec_sorted\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a722a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assuming RT_sec is already aligned with the epochs (i.e., came from behavior logs matched to these trials)\n",
    "# epochs.metadata = pd.DataFrame({'RT_sec': RT_sec})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94860b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rank_order = np.argsort(epochs.metadata['RT_sec'].values)\n",
    "# RT_sec_sorted = epochs.metadata['RT_sec'].values[rank_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff6c3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Add RTs to metadata\n",
    "epochs.metadata = pd.DataFrame({'RT': RT_sec})\n",
    "\n",
    "# Get sorted indices\n",
    "sorted_indices = np.argsort(epochs.metadata['RT'].values)\n",
    "\n",
    "# Sort the Epochs object using those indices\n",
    "epochs_sorted = epochs[sorted_indices]\n",
    "\n",
    "# Also sort RTs for overlay (though they should now match)\n",
    "RT_sec_sorted = RT_sec[sorted_indices]\n",
    "\n",
    "# Now plot\n",
    "epochs_sorted.plot_image(\n",
    "    picks=raw.ch_names[1],\n",
    "    combine=\"mean\",\n",
    "    overlay_times=RT_sec_sorted,\n",
    "    vmin=-1.5e7,  # set your minimum value (in Volts)\n",
    "    vmax=1.5e7,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ephy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
