{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51ad38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mne\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import functions.utils as utils\n",
    "import functions.io as io\n",
    "import functions.preprocessing as preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebff645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Session to preprocess\n",
    "session_id = \"sub027 DBS OFF mSST\"\n",
    "sub = session_id.split(' ') [0]\n",
    "condition = session_id.split(' ') [1] + ' ' + session_id.split(' ') [2]\n",
    "\n",
    "working_path = os.path.dirname(os.getcwd())\n",
    "onedrive_path = utils._get_onedrive_path()\n",
    "\n",
    "sub_onedrive_path_task = os.path.join(onedrive_path, sub, 'synced_data', session_id)\n",
    "\n",
    "#  Set saving and source paths\n",
    "results_path = os.path.join(working_path, \"results\")\n",
    "saving_path = os.path.join(results_path, session_id)\n",
    "source_path = os.path.join(saving_path, \"data\")\n",
    "file_p = os.path.join(source_path, f\"{sub}_postICA_EEGdata_eeg.fif\")\n",
    "fig_saving_path = os.path.join(saving_path, \"figures\")\n",
    "if not os.path.exists(fig_saving_path):\n",
    "    os.makedirs(fig_saving_path)\n",
    "\n",
    "# Load fif file\n",
    "raw = mne.io.read_raw_fif(file_p, preload=True)\n",
    "ch_names = raw.info['ch_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8c2707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eeg_data = raw.get_data(picks='eeg')\n",
    "# eeg_times = raw.times\n",
    "# print(eeg_times.shape)  # (n_times,)\n",
    "\n",
    "# eeg_data.shape  # (n_channels, n_times)\n",
    "# print(eeg_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e89bd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ch_names.remove('BIP 02')\n",
    "# ch_names.remove('BIP 03')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a91dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# for ch in ch_names:\n",
    "#     ch_index = ch_names.index(ch)\n",
    "#     plt.figure(figsize=(10, 4))\n",
    "#     plt.plot(eeg_times, eeg_data[ch_index, :])\n",
    "#     plt.title(f'EEG Channel: {ch}')\n",
    "#     plt.xlabel('Time (s)')\n",
    "#     plt.ylabel('Amplitude (V)')\n",
    "#     plt.xlim(eeg_times[0], eeg_times[-1])\n",
    "#     plt.grid()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ee645e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mSST_raw_behav_session_data_path = os.path.join(\n",
    "        onedrive_path, sub, \"raw_data\", 'BEHAVIOR', condition, 'mSST'\n",
    "        )\n",
    "for filename in os.listdir(mSST_raw_behav_session_data_path):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            fname = filename\n",
    "filepath_behav = os.path.join(mSST_raw_behav_session_data_path, fname)\n",
    "df = pd.read_csv(filepath_behav)\n",
    "\n",
    "# return the index of the first row which is not filled by a Nan value:\n",
    "start_task_index = df['blocks.thisRepN'].first_valid_index()\n",
    "stop_task_index = df['blocks.thisRepN'].last_valid_index()\n",
    "df_maintask = df.iloc[start_task_index:stop_task_index + 1] ### HERE MISTAKE OF INDEXING: CHECK IN OTHER SCRIPTS IF THIS IS ALSO WRONG!!!\n",
    "\n",
    "\n",
    "# remove all useless columns to clean up dataframe\n",
    "column_names = df_maintask.columns\n",
    "columns_to_keep = [i for i in [\n",
    "    'blocks.thisN', 'trial_loop.thisN', 'trial_type', \n",
    "    'continue_signal_time', 'stop_signal_time', \n",
    "    'fixation_cross.started', 'go_rectangle.started',\n",
    "    'key_resp_experiment.keys', 'key_resp_experiment.corr', 'key_resp_experiment.rt',\n",
    "    'early_press_resp.keys', 'early_press_resp.rt', 'early_press_resp.corr',\n",
    "    'late_key_resp1.keys', 'late_key_resp1.rt', \n",
    "    'late_key_resp2.keys', 'late_key_resp2.rt'\n",
    "    ] if i in column_names]\n",
    "\n",
    "mini_df_maintask = df_maintask[columns_to_keep]\n",
    "\n",
    "# remove the trials with early presses, as in these trials the cues were not presented (for mSST)\n",
    "early_presses = mini_df_maintask[mini_df_maintask['early_press_resp.corr'] == 1]\n",
    "early_presses_trials = list(early_presses.index)\n",
    "number_early_presses = len(early_presses_trials)\n",
    "\n",
    "# remove trials with early presses from the dataframe:\n",
    "df_maintask_copy = mini_df_maintask.drop(early_presses_trials).reset_index(drop=True)\n",
    "\n",
    "# First generate global epochs (without taking into account success outcome)\n",
    "# events and event_id used for epochs creation\n",
    "events, event_id = mne.events_from_annotations(raw)\n",
    "epochs, filtered_event_dict = preprocessing.create_epochs(\n",
    "        raw, \n",
    "        sub, \n",
    "        keys_to_keep = ['GC', 'GF', 'GO', 'GS', 'continue', 'stop'],\n",
    "        tmin = -3.5,\n",
    "        tmax = 3.5,\n",
    "        baseline=None\n",
    "        )\n",
    "n_epochs = len(epochs)\n",
    "\n",
    "# inverse mapping (event code -> label)\n",
    "inv_event_id = {v: k for k, v in event_id.items()}\n",
    "\n",
    "metadata = pd.DataFrame(index=np.arange(len(epochs)))\n",
    "metadata[\"event\"] = [inv_event_id[e] for e in epochs.events[:, 2]]\n",
    "metadata[\"sample\"] = epochs.events[:, 0]\n",
    "metadata[\"event_timing\"] = epochs.events[:, 0] / raw.info['sfreq']  # in seconds\n",
    "metadata[\"trial_type\"] = np.nan\n",
    "\n",
    "# LFP -> behavioral naming mapping\n",
    "mapping = {\n",
    "    \"GC\": \"go_continue_trial\",\n",
    "    \"GO\": \"go_trial\",\n",
    "    \"GF\": \"go_fast_trial\",\n",
    "    \"GS\": \"stop_trial\",\n",
    "}\n",
    "\n",
    "trial_mask = metadata[\"event\"].isin(mapping.keys())\n",
    "\n",
    "assert trial_mask.sum() == len(df_maintask_copy), \\\n",
    "    f\"Mismatch: {trial_mask.sum()} LFP trials vs {len(df_maintask_copy)} behavioral trials\"\n",
    "\n",
    "# fill directly from behavioral file\n",
    "for col in df_maintask_copy.columns:\n",
    "    metadata.loc[trial_mask, col] = df_maintask_copy[col].values\n",
    "\n",
    "for i in metadata.index:\n",
    "    if metadata.loc[i, \"event\"] == \"continue\":\n",
    "        # find the last GC before this\n",
    "        prev_idx = metadata.loc[:i-1][metadata[\"event\"] == \"GC\"].index[-1]\n",
    "        metadata.loc[i, df_maintask_copy.columns] = metadata.loc[prev_idx, df_maintask_copy.columns]\n",
    "\n",
    "    elif metadata.loc[i, \"event\"] == \"stop\":\n",
    "        # find the last GS before this\n",
    "        prev_idx = metadata.loc[:i-1][metadata[\"event\"] == \"GS\"].index[-1]\n",
    "        metadata.loc[i, df_maintask_copy.columns] = metadata.loc[prev_idx, df_maintask_copy.columns]\n",
    "\n",
    "epochs.metadata = metadata\n",
    "\n",
    "# sub_dict_epochs[session_ID] = epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2024c0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b0dc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Implementation of all the FASTER steps.\"\"\"\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import mne\n",
    "import numpy as np\n",
    "import scipy.signal\n",
    "from mne import pick_info\n",
    "from mne._fiff.pick import _picks_by_type\n",
    "from mne.preprocessing.bads import _find_outliers\n",
    "from mne.utils import logger\n",
    "from scipy.stats import kurtosis\n",
    "\n",
    "\n",
    "def _bad_mask_to_names(info, bad_mask):\n",
    "    \"\"\"Remap mask to ch names.\"\"\"\n",
    "    bad_idx = [np.where(m)[0] for m in bad_mask]\n",
    "    return [[info[\"ch_names\"][k] for k in epoch] for epoch in bad_idx]\n",
    "\n",
    "\n",
    "def _combine_indices(bads):\n",
    "    \"\"\"Summarize indices.\"\"\"\n",
    "    return list(set(v for val in bads.values() if len(val) > 0 for v in val))\n",
    "\n",
    "\n",
    "def hurst(x):\n",
    "    \"\"\"Estimate Hurst exponent on a timeseries.\n",
    "\n",
    "    The estimation is based on the second order discrete derivative.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : 1D numpy array\n",
    "        The timeseries to estimate the Hurst exponent for.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    h : float\n",
    "        The estimation of the Hurst exponent for the given timeseries.\n",
    "\n",
    "    \"\"\"\n",
    "    y = np.cumsum(np.diff(x, axis=1), axis=1)\n",
    "\n",
    "    b1 = [1, -2, 1]\n",
    "    b2 = [1, 0, -2, 0, 1]\n",
    "\n",
    "    # second order derivative\n",
    "    y1 = scipy.signal.lfilter(b1, 1, y, axis=1)\n",
    "    y1 = y1[:, len(b1) - 1 : -1]  # first values contain filter artifacts\n",
    "\n",
    "    # wider second order derivative\n",
    "    y2 = scipy.signal.lfilter(b2, 1, y, axis=1)\n",
    "    y2 = y2[:, len(b2) - 1 : -1]  # first values contain filter artifacts\n",
    "\n",
    "    s1 = np.mean(y1**2, axis=1)\n",
    "    s2 = np.mean(y2**2, axis=1)\n",
    "\n",
    "    return 0.5 * np.log2(s2 / s1)\n",
    "\n",
    "\n",
    "def _efficient_welch(data, sfreq):\n",
    "    \"\"\"Call scipy.signal.welch with parameters optimized for greatest speed.\n",
    "\n",
    "    Comes at the expense of precision. The window is set to ~10 seconds and windows are\n",
    "    non-overlapping.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : array, shape (..., n_samples)\n",
    "        The timeseries to estimate signal power for. The last dimension\n",
    "        is assumed to be time.\n",
    "    sfreq : float\n",
    "        The sample rate of the timeseries.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    fs : array of float\n",
    "        The frequencies for which the power spectra was calculated.\n",
    "    ps : array, shape (..., frequencies)\n",
    "        The power spectra for each timeseries.\n",
    "\n",
    "    \"\"\"\n",
    "    from scipy.signal import welch\n",
    "\n",
    "    nperseg = min(data.shape[-1], 2 ** int(np.log2(10 * sfreq) + 1))  # next power of 2\n",
    "\n",
    "    return welch(data, sfreq, nperseg=nperseg, noverlap=0, axis=-1)\n",
    "\n",
    "\n",
    "def _freqs_power(data, sfreq, freqs):\n",
    "    fs, ps = _efficient_welch(data, sfreq)\n",
    "    try:\n",
    "        return np.sum([ps[..., np.searchsorted(fs, f)] for f in freqs], axis=0)\n",
    "    except IndexError:\n",
    "        raise ValueError(\n",
    "            (\n",
    "                \"Insufficient sample rate to  estimate power at {} Hz for line \"\n",
    "                \"noise detection. Use the 'metrics' parameter to disable the \"\n",
    "                \"'line_noise' metric.\"\n",
    "            ).format(freqs)\n",
    "        )\n",
    "\n",
    "\n",
    "def _distance_correction(info, picks, x):\n",
    "    \"\"\"Remove the effect of distance to reference sensor.\n",
    "\n",
    "    Computes the distance of each sensor to the reference sensor. Then regresses the\n",
    "    effect of this distance out of the values in x.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    info : instance of Info\n",
    "        The measurement info. This should contain positions for all the sensors.\n",
    "    picks : list of int\n",
    "        Indices of the channels that correspond to the values in x.\n",
    "    x : list of float\n",
    "        Values to correct.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x_corr : list of float\n",
    "        values in x corrected for the distance to reference sensor.\n",
    "\n",
    "    \"\"\"\n",
    "    pos = np.array([info[\"chs\"][ch][\"loc\"][:3] for ch in picks])\n",
    "    ref_pos = np.array([info[\"chs\"][ch][\"loc\"][3:6] for ch in picks])\n",
    "\n",
    "    if np.any(np.all(pos == 0, axis=1)):\n",
    "        raise ValueError(\n",
    "            \"Cannot perform correction for distance to reference \"\n",
    "            \"sensor: not all selected channels have position \"\n",
    "            \"information.\"\n",
    "        )\n",
    "    if np.any(np.all(ref_pos == 0, axis=1)):\n",
    "        raise ValueError(\n",
    "            \"Cannot perform correction for distance to reference \"\n",
    "            \"sensor: the location of the reference sensor is not \"\n",
    "            \"specified for all selected channels.\"\n",
    "        )\n",
    "\n",
    "    # Compute angular distances to the reference sensor\n",
    "    pos /= np.linalg.norm(pos, axis=1)[:, np.newaxis]\n",
    "    ref_pos /= np.linalg.norm(ref_pos, axis=1)[:, np.newaxis]\n",
    "    angles = [np.arccos(np.dot(a, b)) for a, b in zip(pos, ref_pos)]\n",
    "\n",
    "    # Fit a quadratic curve to correct for the angular distance\n",
    "    fit = np.polyfit(angles, x, 2)\n",
    "    return x - np.polyval(fit, angles)\n",
    "\n",
    "\n",
    "def find_bad_channels(\n",
    "    epochs,\n",
    "    picks=None,\n",
    "    max_iter=1,\n",
    "    thres=3,\n",
    "    eeg_ref_corr=False,\n",
    "    use_metrics=None,\n",
    "    return_by_metric=False,\n",
    "):\n",
    "    \"\"\"Automatically find and mark bad channels.\n",
    "\n",
    "    Implements the first step of the FASTER algorithm.\n",
    "\n",
    "    This function attempts to automatically mark bad EEG channels by performing outlier\n",
    "    detection. It operated on epoched data, to make sure only relevant data is analyzed.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    epochs : Instance of Epochs\n",
    "        The epochs for which bad channels need to be marked\n",
    "    picks : list of int | None\n",
    "        Channels to operate on. Defaults to EEG channels.\n",
    "    thres : float\n",
    "        The threshold value, in standard deviations, to apply. A channel crossing this\n",
    "        threshold value is marked as bad. Defaults to 3.\n",
    "    max_iter : int\n",
    "        The maximum number of iterations performed during outlier detection\n",
    "        (defaults to 1, as in the original FASTER paper).\n",
    "    eeg_ref_corr : bool\n",
    "        If the EEG data has been referenced using a single electrode setting this\n",
    "        parameter to True will enable a correction factor for the distance of each\n",
    "        electrode to the reference. If an average reference is applied, or the mean of\n",
    "        multiple reference electrodes, set this parameter to False. Defaults to False,\n",
    "        which disables the correction.\n",
    "    use_metrics : list of str\n",
    "        List of metrics to use. Can be any combination of:\n",
    "            'variance', 'correlation', 'hurst', 'kurtosis', 'line_noise'\n",
    "        Defaults to all of them.\n",
    "    return_by_metric : bool\n",
    "        Whether to return the bad channels as a flat list (False, default) or as a\n",
    "        dictionary with the names of the used metrics as keys and the bad channels found\n",
    "        by this metric as values.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bads : list of str\n",
    "        The names of the bad EEG channels.\n",
    "\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        \"variance\": lambda x: np.var(x, axis=1),\n",
    "        \"correlation\": lambda x: np.nanmean(\n",
    "            np.ma.masked_array(np.corrcoef(x), np.identity(len(x), dtype=bool)), axis=0\n",
    "        ),\n",
    "        \"hurst\": lambda x: hurst(x),\n",
    "        \"kurtosis\": lambda x: kurtosis(x, axis=1),\n",
    "        \"line_noise\": lambda x: _freqs_power(x, epochs.info[\"sfreq\"], [50, 60]),\n",
    "    }\n",
    "\n",
    "    if picks is None:\n",
    "        picks = mne.pick_types(epochs.info, meg=False, eeg=True, exclude=[])\n",
    "    if use_metrics is None:\n",
    "        use_metrics = metrics.keys()\n",
    "\n",
    "    # Concatenate epochs in time\n",
    "    data = epochs.get_data(copy=False)[:, picks]\n",
    "    data = data.transpose(1, 0, 2).reshape(data.shape[1], -1)\n",
    "\n",
    "    # Find bad channels\n",
    "    bads = defaultdict(list)\n",
    "    info = pick_info(epochs.info, picks, copy=True)\n",
    "    for ch_type, chs in _picks_by_type(info):\n",
    "        logger.info(\"Bad channel detection on %s channels:\" % ch_type.upper())\n",
    "        for metric in use_metrics:\n",
    "            scores = metrics[metric](data[chs])\n",
    "            if eeg_ref_corr:\n",
    "                scores = _distance_correction(epochs.info, picks, scores)\n",
    "            bad_channels = [\n",
    "                epochs.ch_names[picks[chs[i]]]\n",
    "                for i in _find_outliers(scores, thres, max_iter)\n",
    "            ]\n",
    "            logger.info(\"\\tBad by %s: %s\" % (metric, bad_channels))\n",
    "            bads[metric].append(bad_channels)\n",
    "\n",
    "    bads = dict((k, np.concatenate(v).tolist()) for k, v in bads.items())\n",
    "\n",
    "    if return_by_metric:\n",
    "        return bads\n",
    "    else:\n",
    "        return _combine_indices(bads)\n",
    "\n",
    "\n",
    "def _deviation(data):\n",
    "    \"\"\"Compute the deviation from mean for each channel in a set of epochs.\n",
    "\n",
    "    This is not implemented as a lambda function, because the channel means should be\n",
    "    cached during the computation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : 3D numpy array\n",
    "        The epochs (#epochs x #channels x #samples).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dev : list of float\n",
    "        For each epoch, the mean deviation of the channels.\n",
    "\n",
    "    \"\"\"\n",
    "    ch_mean = np.mean(data, axis=2)\n",
    "    return ch_mean - np.mean(ch_mean, axis=0)\n",
    "\n",
    "\n",
    "def find_bad_epochs(\n",
    "    epochs, picks=None, thres=3, max_iter=1, use_metrics=None, return_by_metric=False\n",
    "):\n",
    "    \"\"\"Automatically find and mark bad epochs.\n",
    "\n",
    "    Implements the second step of the FASTER algorithm.\n",
    "\n",
    "    This function attempts to automatically mark bad epochs by performing outlier\n",
    "    detection.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    epochs : Instance of Epochs\n",
    "        The epochs to analyze.\n",
    "    picks : list of int | None\n",
    "        Channels to operate on. Defaults to EEG channels.\n",
    "    thres : float\n",
    "        The threshold value, in standard deviations, to apply. An epoch\n",
    "        crossing this threshold value is marked as bad. Defaults to 3.\n",
    "    max_iter : int\n",
    "        The maximum number of iterations performed during outlier detection\n",
    "        (defaults to 1, as in the original FASTER paper).\n",
    "    use_metrics : list of str\n",
    "        List of metrics to use. Can be any combination of:\n",
    "            'amplitude', 'variance', 'deviation'\n",
    "        Defaults to all of them.\n",
    "    return_by_metric : bool\n",
    "        Whether to return the bad channels as a flat list (False, default) or as a\n",
    "        dictionary with the names of the used metrics as keys and the bad channels found\n",
    "        by this metric as values.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bads : list of int\n",
    "        The indices of the bad epochs.\n",
    "\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        \"amplitude\": lambda x: np.mean(np.ptp(x, axis=2), axis=1),\n",
    "        \"deviation\": lambda x: np.mean(_deviation(x), axis=1),\n",
    "        \"variance\": lambda x: np.mean(np.var(x, axis=2), axis=1),\n",
    "    }\n",
    "\n",
    "    if picks is None:\n",
    "        picks = mne.pick_types(epochs.info, meg=False, eeg=True, exclude=\"bads\")\n",
    "    if use_metrics is None:\n",
    "        use_metrics = metrics.keys()\n",
    "\n",
    "    info = pick_info(epochs.info, picks, copy=True)\n",
    "    data = epochs.get_data(copy=True)[:, picks]\n",
    "\n",
    "    bads = defaultdict(list)\n",
    "    for ch_type, chs in _picks_by_type(info):\n",
    "        #logger.info(\"Bad epoch detection on %s channels:\" % ch_type.upper())\n",
    "        for metric in use_metrics:\n",
    "            scores = metrics[metric](data[:, chs])\n",
    "            bad_epochs = _find_outliers(scores, thres, max_iter)\n",
    "            #logger.info(\"\\tBad by %s: %s\" % (metric, bad_epochs))\n",
    "            bads[metric].append(bad_epochs)\n",
    "\n",
    "    bads = dict((k, np.concatenate(v).tolist()) for k, v in bads.items())\n",
    "    if return_by_metric:\n",
    "        return bads\n",
    "    else:\n",
    "        return _combine_indices(bads)\n",
    "\n",
    "\n",
    "def _power_gradient(data, sfreq, prange):\n",
    "    \"\"\"Estimate the gradient of the power spectrum at upper frequencies.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : array, shape (n_components, n_samples)\n",
    "        The timeseries to estimate signal power for. The last dimension is presumed to\n",
    "        be time.\n",
    "    sfreq : float\n",
    "        The sample rate of the timeseries.\n",
    "    prange : pair of floats\n",
    "        The (lower, upper) frequency limits of the power spectrum to use. In the FASTER\n",
    "        paper, they set these to the passband of the lowpass filter.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    grad : array of float\n",
    "        The gradients of the timeseries.\n",
    "\n",
    "    \"\"\"\n",
    "    fs, ps = _efficient_welch(data, sfreq)\n",
    "\n",
    "    # Limit power spectrum to selected frequencies\n",
    "    start, stop = (np.searchsorted(fs, p) for p in prange)\n",
    "    if start >= ps.shape[1]:\n",
    "        raise ValueError(\n",
    "            (\n",
    "                \"Sample rate insufficient to estimate {} Hz power. \"\n",
    "                \"Use the 'power_gradient_range' parameter to tweak \"\n",
    "                \"the tested frequencies for this metric or use the \"\n",
    "                \"'metrics' parameter to disable the \"\n",
    "                \"'power_gradient' metric.\"\n",
    "            ).format(prange[0])\n",
    "        )\n",
    "    ps = ps[:, start:stop]\n",
    "\n",
    "    # Compute mean gradients\n",
    "    return np.mean(np.diff(ps), axis=1)\n",
    "\n",
    "\n",
    "def find_bad_components(\n",
    "    ica,\n",
    "    epochs,\n",
    "    thres=3,\n",
    "    max_iter=1,\n",
    "    use_metrics=None,\n",
    "    prange=None,\n",
    "    return_by_metric=False,\n",
    "):\n",
    "    \"\"\"Perform the third step of the FASTER algorithm.\n",
    "\n",
    "    This function attempts to automatically mark bad ICA components by performing\n",
    "    outlier detection.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ica : Instance of ICA\n",
    "        The ICA operator, already fitted to the supplied Epochs object.\n",
    "    epochs : Instance of Epochs\n",
    "        The untransformed epochs to analyze.\n",
    "    thres : float\n",
    "        The threshold value, in standard deviations, to apply. A component crossing this\n",
    "        threshold value is marked as bad. Defaults to 3.\n",
    "    max_iter : int\n",
    "        The maximum number of iterations performed during outlier detection\n",
    "        (defaults to 1, as in the original FASTER paper).\n",
    "    use_metrics : list of str\n",
    "        List of metrics to use. Can be any combination of:\n",
    "            'eog_correlation', 'kurtosis', 'power_gradient', 'hurst',\n",
    "            'median_gradient'\n",
    "        Defaults to all of them.\n",
    "    prange : None | pair of floats\n",
    "        The (lower, upper) frequency limits of the power spectrum to use for the power\n",
    "        gradient computation. In the FASTER paper, they set these to the passband of the\n",
    "        highpass and lowpass filter. If None, defaults to the 'highpass' and 'lowpass'\n",
    "        filter settings in ica.info.\n",
    "    return_by_metric : bool\n",
    "        Whether to return the bad channels as a flat list (False, default) or as a\n",
    "        dictionary with the names of the used metrics as keys and the bad channels found\n",
    "        by this metric as values.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bads : list of int\n",
    "        The indices of the bad components.\n",
    "\n",
    "    See Also\n",
    "    --------\n",
    "    ICA.find_bads_ecg\n",
    "    ICA.find_bads_eog\n",
    "\n",
    "    \"\"\"\n",
    "    source_data = ica.get_sources(epochs).get_data(copy=False).transpose(1, 0, 2)\n",
    "    source_data = source_data.reshape(source_data.shape[0], -1)\n",
    "\n",
    "    if prange is None:\n",
    "        prange = (ica.info[\"highpass\"], ica.info[\"lowpass\"])\n",
    "    if len(prange) != 2:\n",
    "        raise ValueError(\"prange must be a pair of floats\")\n",
    "\n",
    "    metrics = {\n",
    "        \"eog_correlation\": lambda x: x.find_bads_eog(epochs)[1],\n",
    "        \"kurtosis\": lambda x: kurtosis(\n",
    "            np.dot(x.mixing_matrix_.T, x.pca_components_[: x.n_components_]), axis=1\n",
    "        ),\n",
    "        \"power_gradient\": lambda x: _power_gradient(\n",
    "            source_data, ica.info[\"sfreq\"], prange\n",
    "        ),\n",
    "        \"hurst\": lambda x: hurst(source_data),\n",
    "        \"median_gradient\": lambda x: np.median(np.abs(np.diff(source_data)), axis=1),\n",
    "        \"line_noise\": lambda x: _freqs_power(\n",
    "            source_data, epochs.info[\"sfreq\"], [50, 60]\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    if use_metrics is None:\n",
    "        use_metrics = metrics.keys()\n",
    "\n",
    "    bads = defaultdict(list)\n",
    "    for metric in use_metrics:\n",
    "        scores = np.atleast_2d(metrics[metric](ica))\n",
    "        for s in scores:\n",
    "            bad_comps = _find_outliers(s, thres, max_iter)\n",
    "            logger.info(\"Bad by %s:\\n\\t%s\" % (metric, bad_comps))\n",
    "            bads[metric].append(bad_comps)\n",
    "\n",
    "    bads = dict((k, np.concatenate(v).tolist()) for k, v in bads.items())\n",
    "    if return_by_metric:\n",
    "        return bads\n",
    "    else:\n",
    "        return _combine_indices(bads)\n",
    "\n",
    "\n",
    "def find_bad_channels_in_epochs(\n",
    "    epochs,\n",
    "    picks=None,\n",
    "    thres=3,\n",
    "    max_iter=1,\n",
    "    eeg_ref_corr=False,\n",
    "    use_metrics=None,\n",
    "    return_by_metric=False,\n",
    "):\n",
    "    \"\"\"Perform the fourth step of the FASTER algorithm.\n",
    "\n",
    "    This function attempts to automatically mark bad channels in each epochs by\n",
    "    performing outlier detection.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    epochs : Instance of Epochs\n",
    "        The epochs to analyze.\n",
    "    picks : list of int | None\n",
    "        Channels to operate on. Defaults to EEG channels.\n",
    "    thres : float\n",
    "        The threshold value, in standard deviations, to apply. An epoch crossing this\n",
    "        threshold value is marked as bad. Defaults to 3.\n",
    "    max_iter : int\n",
    "        The maximum number of iterations performed during outlier detection\n",
    "        (defaults to 1, as in the original FASTER paper).\n",
    "    eeg_ref_corr : bool\n",
    "        If the EEG data has been referenced using a single electrode setting this\n",
    "        parameter to True will enable a correction factor for the distance of each\n",
    "        electrode to the reference. If an average reference is applied, or the mean of\n",
    "        multiple reference electrodes, set this parameter to False. Defaults to False,\n",
    "        which disables the correction.\n",
    "    use_metrics : list of str\n",
    "        List of metrics to use. Can be any combination of:\n",
    "            'amplitude', 'variance', 'deviation', 'median_gradient'\n",
    "        Defaults to all of them.\n",
    "    return_by_metric : bool\n",
    "        Whether to return the bad channels as a flat list (False, default) or as a\n",
    "        dictionary with the names of the used metrics as keys and the bad channels found\n",
    "        by this metric as values.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bads : list of lists of int\n",
    "        For each epoch, the indices of the bad channels.\n",
    "\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        \"amplitude\": lambda x: np.ptp(x, axis=2),\n",
    "        \"deviation\": lambda x: _deviation(x),\n",
    "        \"variance\": lambda x: np.var(x, axis=2),\n",
    "        \"median_gradient\": lambda x: np.median(np.abs(np.diff(x)), axis=2),\n",
    "        \"line_noise\": lambda x: _freqs_power(x, epochs.info[\"sfreq\"], [50, 60]),\n",
    "    }\n",
    "\n",
    "    if picks is None:\n",
    "        picks = mne.pick_types(epochs.info, meg=False, eeg=True, exclude=\"bads\")\n",
    "    if use_metrics is None:\n",
    "        use_metrics = metrics.keys()\n",
    "\n",
    "    info = pick_info(epochs.info, picks, copy=True)\n",
    "    data = epochs.get_data(copy=False)[:, picks]\n",
    "    bads = dict((m, np.zeros((len(data), len(picks)), dtype=bool)) for m in metrics)\n",
    "    for ch_type, chs in _picks_by_type(info):\n",
    "        ch_names = [info[\"ch_names\"][k] for k in chs]\n",
    "        chs = np.array(chs)\n",
    "        for metric in use_metrics:\n",
    "            logger.info(\n",
    "                \"Bad channel-in-epoch detection on %s channels:\" % ch_type.upper()\n",
    "            )\n",
    "            s_epochs = metrics[metric](data[:, chs])\n",
    "            for i_epochs, scores in enumerate(s_epochs):\n",
    "                if eeg_ref_corr:\n",
    "                    scores = _distance_correction(epochs.info, picks, scores)\n",
    "                outliers = _find_outliers(scores, thres, max_iter)\n",
    "                if len(outliers) > 0:\n",
    "                    bad_segment = [ch_names[k] for k in outliers]\n",
    "                    logger.info(\n",
    "                        \"Epoch %d, Bad by %s:\\n\\t%s\" % (i_epochs, metric, bad_segment)\n",
    "                    )\n",
    "                    bads[metric][i_epochs, chs[outliers]] = True\n",
    "\n",
    "    info = pick_info(epochs.info, picks, copy=True)\n",
    "    if return_by_metric:\n",
    "        bads = dict((m, _bad_mask_to_names(info, v)) for m, v in bads.items())\n",
    "    else:\n",
    "        bads = np.sum(list(bads.values()), axis=0).astype(bool)\n",
    "        bads = _bad_mask_to_names(info, bads)\n",
    "\n",
    "    return bads\n",
    "\n",
    "\n",
    "def run_faster(epochs, thres=3, copy=True):\n",
    "    \"\"\"Run the entire FASTER pipeline on the data.\"\"\"\n",
    "    if copy:\n",
    "        epochs = epochs.copy()\n",
    "\n",
    "    # Step one\n",
    "    logger.info(\"Step 1: mark bad channels\")\n",
    "    epochs.info[\"bads\"] += find_bad_channels(epochs, thres=5)\n",
    "\n",
    "    # Step two\n",
    "    logger.info(\"Step 2: mark bad epochs\")\n",
    "    bad_epochs = find_bad_epochs(epochs, thres=thres)\n",
    "    good_epochs = list(set(range(len(epochs))).difference(set(bad_epochs)))\n",
    "    epochs = epochs[good_epochs]\n",
    "\n",
    "    # Step three (using the build-in MNE functionality for this)\n",
    "    logger.info(\"Step 3: mark bad ICA components\")\n",
    "    picks = mne.pick_types(epochs.info, meg=False, eeg=True, eog=True, exclude=\"bads\")\n",
    "    ica = mne.preprocessing.ICA(len(picks)).fit(epochs, picks=picks)\n",
    "    ica.exclude = find_bad_components(ica, epochs, thres=thres)\n",
    "    ica.apply(epochs)\n",
    "    epochs.apply_baseline(epochs.baseline)\n",
    "\n",
    "    # Step four\n",
    "    logger.info(\"Step 4: mark bad channels for each epoch\")\n",
    "    bad_channels_per_epoch = find_bad_channels_in_epochs(epochs, thres=thres)\n",
    "    for i, b in enumerate(bad_channels_per_epoch):\n",
    "        if len(b) > 0:\n",
    "            epoch = epochs[i]\n",
    "            epoch.info[\"bads\"] += b\n",
    "            epoch.interpolate_bads()\n",
    "            epochs._data[i, :, :] = epoch._data[0, :, :]\n",
    "\n",
    "    # Now that the data is clean, apply average reference\n",
    "    epochs.set_eeg_reference(\"average\")\n",
    "\n",
    "    # That's all for now\n",
    "    return epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599d5c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mne._fiff.pick import _picks_by_type\n",
    "from scipy.stats import zscore\n",
    "\n",
    "#epochs = cleaned_epochs.copy()\n",
    "\n",
    "metrics = {\n",
    "    \"amplitude\": lambda x: np.mean(np.ptp(x, axis=2), axis=1),\n",
    "    \"deviation\": lambda x: np.mean(_deviation(x), axis=1),\n",
    "    \"variance\": lambda x: np.mean(np.var(x, axis=2), axis=1),\n",
    "}\n",
    "thresh = 3\n",
    "max_iter = 1\n",
    "tail=0\n",
    "picks = mne.pick_types(epochs.info, meg=False, eeg=True, exclude=\"bads\")\n",
    "use_metrics = metrics.keys()\n",
    "\n",
    "info = pick_info(epochs.info, picks, copy=True)\n",
    "data = epochs.get_data(copy=True)[:, picks]\n",
    "\n",
    "bads={}\n",
    "all_bads = []\n",
    "bads = defaultdict(list)\n",
    "for ch_type, chs in _picks_by_type(info):\n",
    "    #logger.info(\"Bad epoch detection on %s channels:\" % ch_type.upper())\n",
    "    for metric in use_metrics:\n",
    "        scores = metrics[metric](data[:, chs])\n",
    "\n",
    "        ############################################\"\"\n",
    "        my_mask = np.zeros(len(scores), dtype=bool)\n",
    "        for _ in range(max_iter):\n",
    "            scores = np.ma.masked_array(scores, my_mask)\n",
    "            if tail == 0:\n",
    "                this_z = np.abs(zscore(scores))\n",
    "            elif tail == 1:\n",
    "                this_z = zscore(scores)\n",
    "            elif tail == -1:\n",
    "                this_z = -zscore(scores)\n",
    "            else:\n",
    "                raise ValueError(f\"Tail parameter {tail} not recognised.\")\n",
    "            local_bad = this_z > thresh\n",
    "            my_mask = np.max([my_mask, local_bad], 0)\n",
    "            if not np.any(local_bad):\n",
    "                break\n",
    "\n",
    "        bad_epochs = np.where(my_mask)[0]\n",
    "        ##############################################\"\"\n",
    "        \n",
    "        #bad_epochs = _find_outliers(scores, thres, max_iter)\n",
    "        #logger.info(\"\\tBad by %s: %s\" % (metric, bad_epochs))\n",
    "        bads[metric].append(bad_epochs)\n",
    "        all_bads.append(bad_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754acf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: mark bad epochs\n",
    "cleaned_epochs = epochs.copy()\n",
    "bad_epochs = find_bad_epochs(cleaned_epochs, return_by_metric=False)\n",
    "bad_epochs.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d00baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f4dd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "fig = cleaned_epochs.plot(n_epochs=4, n_channels=len(ch_names), events=True, block=True)\n",
    "plt.close(fig)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7216a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738762c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df = pd.DataFrame(cleaned_epochs.metadata)\n",
    "# save both to csv (easier for later python import), and xlsx (easier to read in excel)\n",
    "metadata_df.to_csv(os.path.join(saving_path, f\"{session_id}_cleaned-long-epo_metadata.csv\"), index=True)\n",
    "metadata_df.to_excel(os.path.join(saving_path, f\"{session_id}_cleaned-long-epo_metadata.xlsx\"), index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfca574a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_epoch = os.path.join(results_path, 'eeg_epochs', f\"{session_id}_EEG_cleaned-long-epo.fif\")\n",
    "cleaned_epochs.save(file_epoch, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afb9751",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "# Generate list of evoked objects from conditions names\n",
    "evokeds = [cleaned_epochs[name].crop(tmin=-0.5, tmax=1.5).average() for name in (\"GO\", \"GF\",\"GC\", \"GS\")]\n",
    "colors = \"blue\", \"red\", \"green\", \"black\"\n",
    "title = \"Evoked responses\"\n",
    "\n",
    "fig, axes = plt.subplots(1)\n",
    "\n",
    "mne.viz.plot_evoked_topo(evokeds, title=title, background_color=\"w\", axes=axes)\n",
    "\n",
    "#fig.savefig(join(saving_path, 'evoked_responses.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b339fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "### TFR PARAMETERS ###\n",
    "######################\n",
    "\n",
    "decim = 1 \n",
    "freqs = np.arange(1, 80, 1) \n",
    "# For 500ms time resolution at 1 Hz: n_cycles = 1 * 0.5 = 0.5\n",
    "# For 50ms time resolution at 40 Hz: n_cycles = 40 * 0.05 = 2\n",
    "# Linear interpolation between these points\n",
    "#n_cycles = 0.5 + (freqs - 1) * (2 - 0.5) / (40 - 1)\n",
    "#n_cycles = freqs / 2.0\n",
    "n_cycles = np.minimum(np.maximum(freqs / 2.0, 2), 10)\n",
    "\n",
    "tfr_args = dict(\n",
    "    method=\"morlet\",\n",
    "    freqs=freqs,\n",
    "    n_cycles=n_cycles,\n",
    "    decim=decim,\n",
    "    return_itc=False,\n",
    "    average=False\n",
    ")        \n",
    "\n",
    "tmin_tmax = [-500, 1500]\n",
    "vmin_vmax = [-70, 70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226baab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "for epoch_cond in ['GO_successful', 'GF_successful', 'GC_successful', 'GS_successful', 'GS_unsuccessful']:\n",
    "    ch_interest = \"Cz\"\n",
    "\n",
    "    t_min_max = [-500, 1500]\n",
    "\n",
    "    epoch_type = epoch_cond.split('_')[0]\n",
    "    outcome_str = epoch_cond.split('_')[1]\n",
    "\n",
    "    outcome = 1.0 if outcome_str == 'successful' else 0.0\n",
    "\n",
    "    type_mask = cleaned_epochs.metadata[\"event\"] == epoch_type\n",
    "    outcome_mask = cleaned_epochs.metadata[\"key_resp_experiment.corr\"] == outcome\n",
    "    data = cleaned_epochs[type_mask & outcome_mask]\n",
    "\n",
    "    channel_epochs = data.copy().pick([ch_interest])\n",
    "    power_channel = channel_epochs.compute_tfr(**tfr_args)\n",
    "    mean_power_channel = np.nanmean(power_channel.data, axis=0).squeeze()\n",
    "\n",
    "    times = power_channel.times * 1000  # Convert to milliseconds\n",
    "    freqs = power_channel.freqs\n",
    "\n",
    "    baseline_indices = (times >= -500) & (times <= -200)\n",
    "    baseline_power_channel = np.nanmean(mean_power_channel[:, baseline_indices], axis=1, keepdims=True)\n",
    "    percentage_change_channel = (mean_power_channel - baseline_power_channel) / baseline_power_channel * 100\n",
    "\n",
    "    time_indices = np.logical_and(times >= t_min_max[0], times <= t_min_max[1])\n",
    "    sliced_data = percentage_change_channel[:, time_indices].squeeze()    \n",
    "\n",
    "    plt.imshow(sliced_data, aspect='auto', origin='lower', \n",
    "            extent=[t_min_max[0], t_min_max[1], \n",
    "            tfr_args[\"freqs\"][0], tfr_args[\"freqs\"][-1]], \n",
    "            cmap='jet', vmin=vmin_vmax[0], vmax=vmin_vmax[-1]\n",
    "    )\n",
    "    plt.axvline(x=0, color='k', linestyle='--', linewidth=1)\n",
    "    plt.title(f\"Percentage Change in Power for {epoch_cond} - {ch_interest}\")\n",
    "    plt.xlabel(\"Time from GO cue (ms)\")\n",
    "    plt.ylabel(\"Frequency (Hz)\")\n",
    "    plt.colorbar(label=\"Percentage Change (%)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ephy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
